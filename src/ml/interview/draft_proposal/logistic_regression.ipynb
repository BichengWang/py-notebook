{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L4 ~ L5a ML interview coding round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # initialize weights and bias to zeros\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # gradient descent optimization\n",
    "        for i in range(self.n_iters):\n",
    "            # calculate predicted probabilities and cost\n",
    "            z = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = self._sigmoid(z)\n",
    "            cost = (-1 / n_samples) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "            \n",
    "            # calculate gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "            \n",
    "            # update weights and bias\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "    def predict(self, X):\n",
    "        # calculate predicted probabilities\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = self._sigmoid(z)\n",
    "        # convert probabilities to binary predictions\n",
    "        return np.round(y_pred).astype(int)\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "# create sample dataset\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "\n",
    "# initialize logistic regression model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# train model on sample dataset\n",
    "lr.fit(X, y)\n",
    "\n",
    "# make predictions on new data\n",
    "X_new = np.array([[6, 7], [7, 8]])\n",
    "y_pred = lr.predict(X_new)\n",
    "\n",
    "print(y_pred)  # [1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Follow Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add regularization: Regularization can help prevent overfitting and improve the generalization performance of the model. You could add L1 or L2 regularization to the cost function and adjust the regularization strength with a hyperparameter. Here's an example of how to add L2 regularization to the code;\n",
    "\n",
    "Use a more sophisticated optimization algorithm: Gradient descent is a simple and effective optimization algorithm, but it may not be the most efficient or accurate for large or complex datasets. You could try using a more sophisticated algorithm, such as stochastic gradient descent (SGD), mini-batch SGD, or Adam, which can converge faster and find better optima. Here's an example of how to use mini-batch SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000, regularization='l2', reg_strength=0.1, batch_size=32):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.regularization = regularization\n",
    "        self.reg_strength = reg_strength\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        n_batches = n_samples // self.batch_size\n",
    "        for i in range(self.n_iters):\n",
    "            batch_indices = np.random.choice(n_samples, self.batch_size)\n",
    "            X_batch = X[batch_indices]\n",
    "            y_batch = y[batch_indices]\n",
    "            z = np.dot(X_batch, self.weights) + self.bias\n",
    "            y_pred = self._sigmoid(z)\n",
    "            cost = (-1 / self.batch_size) * np.sum(y_batch * np.log(y_pred) + (1 - y_batch) * np.log(1 - y_pred))\n",
    "            if self.regularization == 'l2':\n",
    "                reg_cost = (self.reg_strength / (2 * n_samples)) * np.sum(self.weights ** 2)\n",
    "                cost += reg_cost\n",
    "            elif self.regularization == 'l1':\n",
    "                reg_cost = (self.reg_strength / (2 * n_samples)) * np.sum(np.abs(self.weights))\n",
    "                cost += reg_cost\n",
    "            dw = (1 / self.batch_size) * np.dot(X_batch.T, (y_pred - y_batch))\n",
    "            db = (1 / self.batch_size) * np.sum(y_pred - y_batch)\n",
    "            if self.regularization == 'l2':\n",
    "                dw += (self.reg_strength / n_samples) * self.weights\n",
    "            elif self.regularization == 'l1':\n",
    "                dw += (self.reg_strength / n_samples) * np.sign(self.weights)\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = self._sigmoid(z)\n",
    "        return np.round(y_pred).astype(int)\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization: You can choose between L1 or L2 regularization by setting the regularization parameter to either 'l1' or 'l2', and adjust the regularization strength with the reg_strength parameter.\n",
    "\n",
    "Mini-batch stochastic gradient descent: The model uses mini-batch SGD (instead of simple gradient descent) to update the weights and bias, which can converge faster and find better optima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "# create sample dataset\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "\n",
    "# initialize logistic regression model\n",
    "lr = LogisticRegression(learning_rate=0.01, n_iters=1000, regularization='l2', reg_strength=0.1, batch_size=2)\n",
    "\n",
    "# train model on sample dataset\n",
    "lr.fit(X, y)\n",
    "\n",
    "# make predictions on new data\n",
    "X_new = np.array([[6, 7], [7, 8]])\n",
    "y_pred = lr.predict(X_new)\n",
    "\n",
    "print(y_pred)  # [1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is difficult to visualize logistic regression since it is a high-dimensional problem. However, we can visualize the decision boundary of a logistic regression model for a two-dimensional dataset.\n",
    "\n",
    "Here's an example of how to visualize the decision boundary of the LogisticRegression class on a 2D dataset using the matplotlib library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create 2D dataset\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "\n",
    "# initialize logistic regression model\n",
    "lr = LogisticRegression(learning_rate=0.01, n_iters=1000, regularization='l2', reg_strength=0.1, batch_size=2)\n",
    "\n",
    "# train model on dataset\n",
    "lr.fit(X, y)\n",
    "\n",
    "# plot decision boundary\n",
    "x1 = np.linspace(0, 6, 100)\n",
    "x2 = np.linspace(0, 8, 100)\n",
    "xx, yy = np.meshgrid(x1, x2)\n",
    "Z = lr.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "\n",
    "# plot data points\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.Spectral)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Dimension Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "n, k, p=100, 8, 3 \n",
    "X=np.random.random([n,k])\n",
    "W=np.random.random([k,p])\n",
    "\n",
    "y=np.random.randint(p, size=(1,n))\n",
    "Y=np.zeros((n,p))\n",
    "Y[np.arange(n), y]=1\n",
    "\n",
    "max_itr=5000\n",
    "alpha=0.01\n",
    "Lambda=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F(x)= w[0]*x + w[1]\n",
    "def F(X, W):\n",
    "    return np.matmul(X,W)\n",
    "\n",
    "def H(F):\n",
    "    return 1/(1+np.exp(-F))\n",
    "\n",
    "def cost(Y_est, Y):\n",
    "    E= - (1/n) * (np.sum(Y*np.log(Y_est) + (1-Y)*np.log(1-Y_est)))  + np.linalg.norm(W,2)\n",
    "    return E, np.sum(np.argmax(Y_est,1)==y)/n\n",
    "\n",
    "def gradient(Y_est, Y, X):\n",
    "    return (1/n) * np.matmul(X.T, (Y_est - Y) ) + Lambda* 2* W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(W, X, Y, alpha, max_itr):\n",
    "    for i in range(max_itr):\n",
    "        \n",
    "        F_x=F(X,W)\n",
    "        Y_est=H(F_x)\n",
    "        E, c= cost(Y_est, Y)\n",
    "        Wg=gradient(Y_est, Y, X)\n",
    "        W=W - alpha * Wg\n",
    "        if i%1000==0:\n",
    "            print(E, c)\n",
    "        \n",
    "    return W, Y_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.517344796908038 0.33\n",
      "4.910723800793214 0.4\n",
      "4.884253895909955 0.46\n",
      "4.868185220476246 0.48\n",
      "4.858843958387878 0.49\n"
     ]
    }
   ],
   "source": [
    "X=np.concatenate( (X, np.ones((n,1))), axis=1 ) \n",
    "W=np.concatenate( (W, np.random.random((1,p)) ), axis=0 )\n",
    "\n",
    "W, Y_est = fit(W, X, Y, alpha, max_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
