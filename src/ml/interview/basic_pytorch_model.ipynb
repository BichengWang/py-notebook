{"cells":[{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[],"source":["from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[],"source":["class ResidualBlock(nn.Module):\n","    def __init__(self, in_features, out_features):\n","        super(ResidualBlock, self).__init__()\n","        self.fc1 = nn.Linear(in_features, out_features)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(out_features, out_features)\n","        if in_features != out_features:\n","            self.residual = nn.Linear(in_features, out_features)\n","        else:\n","            self.residual = nn.Identity()\n","\n","    def forward(self, x):\n","        out = self.fc1(x)\n","        out = self.relu(out)\n","        out = self.fc2(out)\n","        # Add the residual\n","        out += self.residual(x)\n","        out = self.relu(out)\n","        return out"]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[],"source":["class ResidualMLP(nn.Module):\n","    def __init__(self, in_features = 28*28, out_features=10, hidden_size=128, resN = 2):\n","        super(ResidualMLP, self).__init__()\n","        self.in_features = in_features\n","        self.fc1 = nn.Linear(in_features, hidden_size)  # Input layer\n","        self.residual_blocks = nn.ModuleList([ResidualBlock(hidden_size, hidden_size) for _ in range(resN)])  # n Residual blocks\n","        self.fc2 = nn.Linear(hidden_size, hidden_size)  # Output layer\n","        self.relu = nn.ReLU()\n","        self.fc3 = nn.Linear(hidden_size, out_features)\n","\n","    def forward(self, x):\n","        x = x.view(-1, self.in_features)  # Flatten the input\n","        x = self.fc1(x)  # First fully connected layer\n","        # Pass through the residual blocks\n","        for block in self.residual_blocks:\n","            x = block(x)\n","        x = self.fc2(x)  # Second fully connected layer\n","        return x"]},{"cell_type":"code","execution_count":110,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/1], Loss: 0.41994240821233947\n","Test Accuracy: 91.99%\n"]}],"source":["\n","# MNIST dataset loading and transformations\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","\n","train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","\n","train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)\n","\n","in_features = 28 * 28\n","model = ResidualMLP()\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","num_epochs = 1\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for batch_idx, (data, targets) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        data = data.view(-1, in_features).float()  # Flatten images into vectors of size 784\n","        outputs = model(data)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n","\n","# Test the model\n","model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data, targets in test_loader:\n","        data = data.view(-1, 28 * 28).float()  # Flatten the input images\n","        outputs = model(data)\n","        _, predicted = torch.max(outputs, 1)\n","        total += targets.size(0)\n","        correct += (predicted == targets).sum().item()\n","\n","print(f'Test Accuracy: {100 * correct / total}%')"]},{"cell_type":"code","execution_count":111,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","\n","class Trainer:\n","    def __init__(self, model, train_loader, test_loader, criterion, optimizer, device='cpu'):\n","        self.model = model.to(device)\n","        self.train_loader = train_loader\n","        self.test_loader = test_loader\n","        self.criterion = criterion\n","        self.optimizer = optimizer\n","        self.device = device\n","\n","    def train(self, num_epochs=5):\n","        self.model.train()\n","        for epoch in range(num_epochs):\n","            running_loss = 0.0\n","            correct = 0\n","            total = 0\n","            \n","            for batch_idx, (data, targets) in enumerate(self.train_loader):\n","                # Move data to the specified device\n","                data, targets = data.to(self.device), targets.to(self.device)\n","                data = data.view(-1, 28*28).float()  # Flatten MNIST image data\n","\n","                # Forward pass\n","                outputs = self.model(data)\n","                loss = self.criterion(outputs, targets)\n","\n","                # Backward pass and optimization\n","                self.optimizer.zero_grad()\n","                loss.backward()\n","                self.optimizer.step()\n","\n","                running_loss += loss.item()\n","                \n","                # Calculate accuracy for the current batch\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += targets.size(0)\n","                correct += (predicted == targets).sum().item()\n","\n","            # Print loss and accuracy for the epoch\n","            epoch_loss = running_loss / len(self.train_loader)\n","            epoch_accuracy = 100 * correct / total\n","            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n","\n","    def evaluate(self):\n","        self.model.eval()\n","        correct = 0\n","        total = 0\n","        test_loss = 0.0\n","        \n","        with torch.no_grad():\n","            for data, targets in self.test_loader:\n","                # Move data to the specified device\n","                data, targets = data.to(self.device), targets.to(self.device)\n","                data = data.view(-1, 28*28).float()  # Flatten MNIST image data\n","                \n","                outputs = self.model(data)\n","                loss = self.criterion(outputs, targets)\n","                test_loss += loss.item()\n","\n","                # Get the predicted class with the highest score\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += targets.size(0)\n","                correct += (predicted == targets).sum().item()\n","\n","        accuracy = 100 * correct / total\n","        avg_loss = test_loss / len(self.test_loader)\n","        print(f'Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n","        return avg_loss, accuracy\n","\n","    def save_checkpoint(self, path='model.pth'):\n","        torch.save(self.model.state_dict(), path)\n","        print(f'Model checkpoint saved at {path}')\n","\n","    def load_checkpoint(self, path='model.pth'):\n","        self.model.load_state_dict(torch.load(path))\n","        print(f'Model checkpoint loaded from {path}')"]},{"cell_type":"code","execution_count":112,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/5], Loss: 0.4729, Accuracy: 87.41%\n","Epoch [2/5], Loss: 0.2120, Accuracy: 93.65%\n","Epoch [3/5], Loss: 0.1823, Accuracy: 94.57%\n","Epoch [4/5], Loss: 0.1839, Accuracy: 94.54%\n","Epoch [5/5], Loss: 0.1834, Accuracy: 94.67%\n","Test Loss: 0.1615, Test Accuracy: 95.31%\n","Model checkpoint saved at residual_mlp_mnist.pth\n","Model checkpoint loaded from residual_mlp_mnist.pth\n"]},{"name":"stderr","output_type":"stream","text":["/var/folders/ls/5qzq87350jz6pv6jwnpg6mkm0000gn/T/ipykernel_47479/1790312051.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  self.model.load_state_dict(torch.load(path))\n"]}],"source":["# Model, loss function, optimizer\n","input_size = 28 * 28  # MNIST images are 28x28 pixels\n","hidden_size = 128  # First hidden layer size\n","output_size = 10    # Output size (10 classes for digits 0-9)\n","num_blocks = 3      # Number of residual blocks\n","\n","model = ResidualMLP(in_features=input_size, out_features=output_size, hidden_size=hidden_size, resN=num_blocks)\n","\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\n","# Instantiate the trainer\n","device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')\n","trainer = Trainer(model, train_loader, test_loader, criterion, optimizer, device)\n","\n","# Train the model\n","trainer.train(num_epochs=5)\n","\n","# Evaluate the model\n","trainer.evaluate()\n","\n","# Save the model checkpoint\n","trainer.save_checkpoint('residual_mlp_mnist.pth')\n","\n","# Load the model checkpoint\n","trainer.load_checkpoint('residual_mlp_mnist.pth')"]},{"cell_type":"code","execution_count":113,"metadata":{},"outputs":[],"source":["class Evaluator:\n","    def __init__(self, model, test_loader, criterion, device='cpu'):\n","        self.model = model.to(device)\n","        self.test_loader = test_loader\n","        self.criterion = criterion\n","        self.device = device\n","    \n","    def evaluate(self):\n","        \"\"\"\n","        Evaluates the model on the test/validation set, computing the accuracy and loss.\n","        \"\"\"\n","        self.model.eval()  # Set the model to evaluation mode\n","        correct = 0\n","        total = 0\n","        test_loss = 0.0\n","        \n","        with torch.no_grad():  # Disable gradient calculation\n","            for data, targets in self.test_loader:\n","                # Move data and targets to the device (CPU or GPU)\n","                data, targets = data.to(self.device), targets.to(self.device)\n","                data = data.view(-1, 28*28).float()  # Flatten input for MLP\n","                \n","                # Forward pass\n","                outputs = self.model(data)\n","                loss = self.criterion(outputs, targets)\n","                test_loss += loss.item()\n","                \n","                # Get the predicted class\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += targets.size(0)\n","                correct += (predicted == targets).sum().item()\n","        \n","        # Calculate average loss and accuracy\n","        accuracy = 100 * correct / total\n","        avg_loss = test_loss / len(self.test_loader)\n","        print(f'Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n","        return avg_loss, accuracy\n","\n","    def predict(self, data):\n","        \"\"\"\n","        Generates predictions for a given batch of data.\n","        \"\"\"\n","        self.model.eval()  # Set the model to evaluation mode\n","        data = data.to(self.device)\n","        data = data.view(-1, 28*28).float()  # Flatten input if necessary\n","        \n","        with torch.no_grad():  # Disable gradient calculation\n","            outputs = self.model(data)\n","            _, predicted = torch.max(outputs.data, 1)\n","        \n","        return predicted"]},{"cell_type":"code","execution_count":114,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/ls/5qzq87350jz6pv6jwnpg6mkm0000gn/T/ipykernel_47479/2969355794.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load('residual_mlp_mnist.pth'))\n"]},{"name":"stdout","output_type":"stream","text":["Test Loss: 0.1615, Test Accuracy: 95.31%\n","Predictions for a batch: tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9], device='mps:0')\n"]}],"source":["# Define the model (same as the model used during training)\n","input_size = 28 * 28\n","hidden_size = 128\n","output_size = 10\n","num_blocks = 3\n","\n","model = ResidualMLP(in_features=input_size, out_features=output_size, hidden_size=hidden_size, resN=num_blocks)\n","\n","# Load the trained model (if available)\n","model.load_state_dict(torch.load('residual_mlp_mnist.pth'))\n","\n","# Define the loss function (same as during training)\n","criterion = nn.CrossEntropyLoss()\n","\n","# Instantiate the evaluator\n","device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')\n","evaluator = Evaluator(model, test_loader, criterion, device)\n","\n","# Evaluate the model\n","_, _ = evaluator.evaluate()\n","\n","# Predict a batch of data from the test set\n","test_batch, _ = next(iter(test_loader))  # Get one batch of test data\n","predictions = evaluator.predict(test_batch)\n","print(f'Predictions for a batch: {predictions[:10]}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"python-notebook","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":2}
