{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Layer NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation:\n",
    "Z1 = X.W1 + b1\n",
    "A1 = ReLU(Z1)  \n",
    "Z2 = A1.W2 + b2   \n",
    "exp_scores = exp(Z2)  \n",
    "probs = exp_scores / sum(exp_scores)\n",
    "\n",
    "### Backward propagation:\n",
    "delta3 = probs   \n",
    "delta3[range(len(X)), y] -= 1   \n",
    "dW2 = A1.T.dot(delta3)   \n",
    "db2 = sum(delta3)   \n",
    "delta2 = delta3.dot(W2.T) * (A1 > 0)   \n",
    "dW1 = X.T.dot(delta2)   \n",
    "db1 = sum(delta2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "class NN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation='relu', lr=0.01):\n",
    "        self.W1 = np.random.randn(input_size, hidden_size)\n",
    "        self.W2 = np.random.randn(hidden_size, output_size)\n",
    "        self.b1 = np.random.randn(hidden_size)\n",
    "        self.b2 = np.random.randn(output_size)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def activation_func(self, z, activation='relu'):\n",
    "        if activation == 'relu':\n",
    "            return np.maximum(0, z)\n",
    "        elif activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        elif activation == 'tanh':\n",
    "            return np.tanh(z)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        z1 = np.dot(X, self.W1) + self.b1\n",
    "        a1 = self.activation_func(z1, self.activation) \n",
    "        z2 = np.dot(a1, self.W2) + self.b2\n",
    "        a2 = self.activation_func(z2, 'sigmoid')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        z1 = np.dot(X, W1) + b1\n",
    "        a1 = np.maximum(0, z1) # ReLU activation function\n",
    "        z2 = np.dot(a1, W2) + b2\n",
    "        # probs = 1 / (1 + np.exp(-z2)) # Sigmoid activation function\n",
    "        exp_z = np.exp(z2)\n",
    "        probs = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "        return probs\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        probs = self.forward(X)\n",
    "        correct_logprobs = -np.log(probs[range(len(X)), y])\n",
    "        data_loss = np.sum(correct_logprobs)\n",
    "        return 1.0/len(X) * data_loss\n",
    "\n",
    "    def train(self, X, y, num_epochs, learning_rate=0.1):\n",
    "        for epoch in range(num_epochs):\n",
    "            # Forward propagation\n",
    "            z1 = np.dot(X, self.params['W1']) + self.params['b1']\n",
    "            a1 = np.maximum(0, z1) # ReLU activation function\n",
    "            z2 = np.dot(a1, self.params['W2']) + self.params['b2']\n",
    "            # probs = 1 / (1 + np.exp(-z2)) # Sigmoid activation function\n",
    "            exp_z = np.exp(z2)\n",
    "            probs = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "            # Backpropagation\n",
    "            delta3 = probs\n",
    "            delta3[range(len(X)), y] -= 1\n",
    "            dW2 = np.dot(a1.T, delta3)\n",
    "            db2 = np.sum(delta3, axis=0)\n",
    "            delta2 = np.dot(delta3, self.params['W2'].T) * (a1 > 0) # derivative of ReLU\n",
    "            dW1 = np.dot(X.T, delta2)\n",
    "            db1 = np.sum(delta2, axis=0)\n",
    "\n",
    "            # Update parameters\n",
    "            self.params['W1'] -= learning_rate * dW1\n",
    "            self.params['b1'] -= learning_rate * db1\n",
    "            self.params['W2'] -= learning_rate * dW2\n",
    "            self.params['b2'] -= learning_rate * db2\n",
    "\n",
    "            # Print loss for monitoring training progress\n",
    "            if epoch % 100 == 0:\n",
    "                loss = self.loss(X, y)\n",
    "                print(\"Epoch {}: loss = {}\".format(epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params definition\n",
    "input_size = 2\n",
    "hidden_size = 10\n",
    "output_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 0.9556106562089086\n",
      "Epoch 100: loss = 0.08039370954169635\n",
      "Epoch 200: loss = 0.026299533066897248\n",
      "Epoch 300: loss = 0.014811125174776101\n",
      "Epoch 400: loss = 0.010067949022975113\n",
      "Epoch 500: loss = 0.0075615969979532845\n",
      "Epoch 600: loss = 0.0060023244854278904\n",
      "Epoch 700: loss = 0.004960782799342221\n",
      "Epoch 800: loss = 0.00421438903840939\n",
      "Epoch 900: loss = 0.00364801332349237\n",
      "Predictions:  [0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Generate a toy dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Initialize a neural network\n",
    "net = TwoLayerNet(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n",
    "\n",
    "# Train the neural network\n",
    "net.train(X, y, num_epochs=1000)\n",
    "\n",
    "# Test the neural network\n",
    "probs = net.forward(X)\n",
    "predictions = np.argmax(probs, axis=1)\n",
    "print(\"Predictions: \", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow-ups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight initialization: The current implementation initializes the weights randomly using a Gaussian distribution. However, it is recommended to use other weight initialization methods such as Xavier or He initialization to improve convergence and avoid vanishing or exploding gradients. One possible implementation for Xavier initialization of the weights is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xavier initialization\n",
    "class CustomizedModel(TwoLayerNet):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__(input_size, hidden_size, output_size)\n",
    "        self.params['W1'] = np.random.randn(input_size, hidden_size) / np.sqrt(input_size)\n",
    "        self.params['W2'] = np.random.randn(hidden_size, output_size) / np.sqrt(hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['b2'] = np.zeros(output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate decay: The learning rate is a hyperparameter that determines the step size at each iteration during training. However, using a fixed learning rate may lead to suboptimal performance or slow convergence. A common technique is to gradually decrease the learning rate over time, known as learning rate decay, to fine-tune the network weights as the optimization process progresses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate decay\n",
    "learning_rate = 0.1\n",
    "lr_decay = 0.95\n",
    "lr_decay_epoch = 100\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # ...\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        learning_rate *= lr_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization: Overfitting can occur when the model is too complex and the training data is limited. Regularization techniques such as L1 or L2 regularization can be applied to the loss function to prevent overfitting and improve the generalization performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 regularization\n",
    "\n",
    "class CustomizedModel(TwoLayerNet):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__(input_size, hidden_size, output_size)\n",
    "        self.reg_lambda = 0.1\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        probs = self.forward(X)\n",
    "        correct_logprobs = -np.log(probs[range(len(X)), y])\n",
    "        data_loss = np.sum(correct_logprobs)\n",
    "        data_loss += 0.5 * self.reg_lambda * (np.sum(self.params['W1'] ** 2) + np.sum(self.params['W2'] ** 2))\n",
    "        return 1.0/len(X) * data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini-batch training: The current implementation updates the weights using the entire training set at each iteration, which can be computationally expensive for large datasets. An alternative is to use mini-batch training, where a random subset of the training data is used at each iteration to update the weights. This can speed up the training process and improve convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch training\n",
    "batch_size = 64\n",
    "num_batches = len(X) // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(num_batches):\n",
    "        # Select a random batch of data\n",
    "        batch_mask = np.random.choice(len(X), batch_size)\n",
    "        X_batch = X[batch_mask]\n",
    "        y_batch = y[batch_mask]\n",
    "\n",
    "        # Forward and backward propagation using the batch data\n",
    "        # ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
