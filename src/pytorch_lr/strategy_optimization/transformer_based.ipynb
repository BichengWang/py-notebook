{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b07b8fec",
   "metadata": {},
   "source": [
    "## Strategy optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1e9f6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "        \"\"\"\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        if src_mask is None:\n",
    "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "            \"\"\"\n",
    "            src_mask = nn.Transformer.generate_square_subsequent_mask(len(src)).to(device)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34fe2028",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a36a2bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n",
      "Requirement already satisfied: torchtext==0.4 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (0.4.0)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from torchtext==0.4) (4.66.4)\n",
      "Requirement already satisfied: requests in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from torchtext==0.4) (2.32.2)\n",
      "Requirement already satisfied: torch in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from torchtext==0.4) (2.4.0.dev20240523)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from torchtext==0.4) (1.26.4)\n",
      "Requirement already satisfied: six in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from torchtext==0.4) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from requests->torchtext==0.4) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from requests->torchtext==0.4) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from requests->torchtext==0.4) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from requests->torchtext==0.4) (2024.2.2)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from torch->torchtext==0.4) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from torch->torchtext==0.4) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from torch->torchtext==0.4) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from torch->torchtext==0.4) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from torch->torchtext==0.4) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from torch->torchtext==0.4) (2024.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from jinja2->torch->torchtext==0.4) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from sympy->torch->torchtext==0.4) (1.3.0)\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m995.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!pip install torchtext==0.4\n",
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26bd2923",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional_datapipe\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IterableWrapper, IterDataPipe\n\u001b[1;32m      8\u001b[0m URL \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m }\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages/torchdata/__init__.py:9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the BSD-style license found in the\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datapipes\n\u001b[1;32m     11\u001b[0m janitor \u001b[38;5;241m=\u001b[39m datapipes\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mjanitor\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages/torchdata/datapipes/__init__.py:9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the BSD-style license found in the\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataChunk, functional_datapipe\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28miter\u001b[39m, \u001b[38;5;28mmap\u001b[39m, utils\n\u001b[1;32m     11\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataChunk\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctional_datapipe\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutils\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages/torchdata/datapipes/iter/__init__.py:54\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceHubReaderIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m HuggingFaceHubReader\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miopath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     49\u001b[0m     IoPathFileListerIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m IoPathFileLister,\n\u001b[1;32m     50\u001b[0m     IoPathFileOpenerIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m IoPathFileOpener,\n\u001b[1;32m     51\u001b[0m     IoPathSaverIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m IoPathSaver,\n\u001b[1;32m     52\u001b[0m )\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     55\u001b[0m     GDriveReaderDataPipe \u001b[38;5;28;01mas\u001b[39;00m GDriveReader,\n\u001b[1;32m     56\u001b[0m     HTTPReaderIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m HttpReader,\n\u001b[1;32m     57\u001b[0m     OnlineReaderIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m OnlineReader,\n\u001b[1;32m     58\u001b[0m )\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01ms3io\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     60\u001b[0m     S3FileListerIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m S3FileLister,\n\u001b[1;32m     61\u001b[0m     S3FileLoaderIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m S3FileLoader,\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbucketbatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     64\u001b[0m     BucketBatcherIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m BucketBatcher,\n\u001b[1;32m     65\u001b[0m     InBatchShufflerIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m InBatchShuffler,\n\u001b[1;32m     66\u001b[0m     MaxTokenBucketizerIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m MaxTokenBucketizer,\n\u001b[1;32m     67\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages/torchdata/datapipes/iter/load/online.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterator, Optional, Tuple\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional_datapipe\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IterDataPipe\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages/requests/__init__.py:48\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RequestsDependencyWarning\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m charset_normalizer_version\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     charset_normalizer_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages/charset_normalizer/__init__.py:23\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mCharset-Normalizer\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m:license: MIT, see LICENSE for more details.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_fp, from_path, from_bytes, normalize\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detect\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__, VERSION\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook/lib/python3.11/site-packages/charset_normalizer/api.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m     PathLike \u001b[38;5;241m=\u001b[39m Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mos.PathLike[str]\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TOO_SMALL_SEQUENCE, TOO_BIG_SEQUENCE, IANA_SUPPORTED\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mess_ratio\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CharsetMatches, CharsetMatch\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "\n",
    "from torchdata.datapipes import functional_datapipe\n",
    "from torchdata.datapipes.iter import IterableWrapper, IterDataPipe\n",
    "\n",
    "\n",
    "URL = {\n",
    "    \"train\": \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\",\n",
    "    \"dev\": \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\",\n",
    "}\n",
    "\n",
    "MD5 = {\n",
    "    \"train\": \"981b29407e0affa3b1b156f72073b945\",\n",
    "    \"dev\": \"3e85deb501d4e538b6bc56f786231552\",\n",
    "}\n",
    "\n",
    "NUM_LINES = {\n",
    "    \"train\": 87599,\n",
    "    \"dev\": 10570,\n",
    "}\n",
    "\n",
    "DATASET_NAME = \"SQuAD1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "580bdf88",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WikiText2\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tokenizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_vocab_from_iterator\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# ``train_iter`` was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Arguments:\n",
    "        data: Tensor, shape ``[N]``\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape ``[N // bsz, bsz]``\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape ``[seq_len, batch_size]``\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a939239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5834b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d169b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29f1b60",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "the batch number of src and tgt must be equal",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m trg \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mtrg\n\u001b[1;32m    102\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 103\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    105\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output_dim)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook2/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook2/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[40], line 70\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     68\u001b[0m src_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mgenerate_square_subsequent_mask(src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mto(src\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     69\u001b[0m trg_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mgenerate_square_subsequent_mask(trg\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mto(trg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 70\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrg_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(output)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook2/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook2/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook2/lib/python3.12/site-packages/torch/nn/modules/transformer.py:210\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    208\u001b[0m is_batched \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe batch number of src and tgt must be equal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m!=\u001b[39m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe batch number of src and tgt must be equal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: the batch number of src and tgt must be equal"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data import Field, BucketIterator, Dataset, Example\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Define Fields for source (expressions) and target (results) sequences\n",
    "SRC = Field(tokenize=lambda x: list(x), init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)\n",
    "TRG = Field(tokenize=lambda x: list(x), init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)\n",
    "\n",
    "# Create a toy dataset of arithmetic expressions\n",
    "data = [\n",
    "    (\"2+2\", \"4\"),\n",
    "    (\"3+5\", \"8\"),\n",
    "    (\"7-3\", \"4\"),\n",
    "    (\"6-1\", \"5\"),\n",
    "    (\"2*3\", \"6\"),\n",
    "    (\"9/3\", \"3\")\n",
    "]\n",
    "\n",
    "# Create Examples and Dataset\n",
    "examples = [Example.fromlist([expr, result], fields=[('src', SRC), ('trg', TRG)]) for expr, result in data]\n",
    "toy_dataset = Dataset(examples, fields=[('src', SRC), ('trg', TRG)])\n",
    "\n",
    "# Build vocabulary\n",
    "SRC.build_vocab(toy_dataset, min_freq=1)\n",
    "TRG.build_vocab(toy_dataset, min_freq=1)\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create iterators\n",
    "BATCH_SIZE = 2\n",
    "train_iterator = BucketIterator(toy_dataset, batch_size=BATCH_SIZE, device=device, shuffle=True)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model=128, nhead=4, num_encoder_layers=2, num_decoder_layers=2, dim_feedforward=512, max_len=100, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.src_embedding = nn.Embedding(input_dim, d_model)\n",
    "        self.trg_embedding = nn.Embedding(output_dim, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src = self.src_embedding(src) * torch.sqrt(torch.tensor(src.size(-1), dtype=torch.float32))\n",
    "        trg = self.trg_embedding(trg) * torch.sqrt(torch.tensor(trg.size(-1), dtype=torch.float32))\n",
    "        src = self.positional_encoding(src)\n",
    "        trg = self.positional_encoding(trg)\n",
    "        src_mask = self.transformer.generate_square_subsequent_mask(src.size(1)).to(src.device)\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(trg.size(1)).to(trg.device)\n",
    "        output = self.transformer(src, trg, src_mask=src_mask, tgt_mask=trg_mask)\n",
    "        output = self.fc_out(output)\n",
    "        return output\n",
    "\n",
    "# Hyperparameters\n",
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "D_MODEL = 128\n",
    "NHEAD = 4\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "NUM_DECODER_LAYERS = 2\n",
    "DIM_FEEDFORWARD = 512\n",
    "DROPOUT = 0.1\n",
    "MAX_LEN = 100\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Initialize the model\n",
    "model = TransformerModel(INPUT_DIM, OUTPUT_DIM, D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, DIM_FEEDFORWARD, MAX_LEN, DROPOUT).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG.vocab.stoi[TRG.pad_token])\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in train_iterator:\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg[:, :-1])\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:, 1:].contiguous().view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {epoch_loss/len(train_iterator):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fcdde5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7867d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model=128, nhead=4, num_encoder_layers=2, num_decoder_layers=2, dim_feedforward=512, max_len=100, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.src_embedding = nn.Embedding(input_dim, d_model)\n",
    "        self.trg_embedding = nn.Embedding(output_dim, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src = self.src_embedding(src) * torch.sqrt(torch.tensor(src.size(-1), dtype=torch.float32))\n",
    "        trg = self.trg_embedding(trg) * torch.sqrt(torch.tensor(trg.size(-1), dtype=torch.float32))\n",
    "        src = self.positional_encoding(src)\n",
    "        trg = self.positional_encoding(trg)\n",
    "        output = self.transformer(src, trg)\n",
    "        output = self.fc_out(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb6e216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/python-notebook2/lib/python3.12/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "the batch number of src and tgt must be equal",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m trg \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mtrg\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 30\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     32\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output_dim)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook2/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook2/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[36], line 29\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     27\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(src)\n\u001b[1;32m     28\u001b[0m trg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(trg)\n\u001b[0;32m---> 29\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(output)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook2/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook2/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook2/lib/python3.12/site-packages/torch/nn/modules/transformer.py:210\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    208\u001b[0m is_batched \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe batch number of src and tgt must be equal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m!=\u001b[39m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe batch number of src and tgt must be equal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: the batch number of src and tgt must be equal"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "D_MODEL = 128\n",
    "NHEAD = 4\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "NUM_DECODER_LAYERS = 2\n",
    "DIM_FEEDFORWARD = 512\n",
    "DROPOUT = 0.1\n",
    "MAX_LEN = 100\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Initialize the model\n",
    "model = TransformerModel(INPUT_DIM, OUTPUT_DIM, D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, DIM_FEEDFORWARD, MAX_LEN, DROPOUT).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG.vocab.stoi[TRG.pad_token])\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in train_iterator:\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg[:, :-1])\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:, 1:].contiguous().view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {epoch_loss/len(train_iterator):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8250ae24",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 4, 32]' is invalid for input of size 384",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     30\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2+2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 31\u001b[0m translation \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSRC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(translation))\n",
      "Cell \u001b[0;32mIn[38], line 18\u001b[0m, in \u001b[0;36mtranslate_sentence\u001b[0;34m(sentence, src_field, trg_field, model, device, max_len)\u001b[0m\n\u001b[1;32m     16\u001b[0m trg_mask \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mgenerate_square_subsequent_mask(trg_tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 18\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositional_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrg_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrg_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_src\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfc_out(output)\n\u001b[1;32m     20\u001b[0m pred_token \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m2\u001b[39m)[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook2/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook2/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook2/lib/python3.12/site-packages/torch/nn/modules/transformer.py:494\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    491\u001b[0m tgt_is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 494\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook2/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook2/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook2/lib/python3.12/site-packages/torch/nn/modules/transformer.py:890\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    888\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x))\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 890\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))\n\u001b[1;32m    891\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))\n\u001b[1;32m    892\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook2/lib/python3.12/site-packages/torch/nn/modules/transformer.py:908\u001b[0m, in \u001b[0;36m_mha_block\u001b[0;34m(self, x, mem, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n\u001b[1;32m    906\u001b[0m \u001b[38;5;66;03m# multihead attention block\u001b[39;00m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mha_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, mem: Tensor,\n\u001b[0;32m--> 908\u001b[0m                attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    909\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultihead_attn(x, mem, mem,\n\u001b[1;32m    910\u001b[0m                             attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m    911\u001b[0m                             key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m    912\u001b[0m                             is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m    913\u001b[0m                             need_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook2/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook2/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook2/lib/python3.12/site-packages/torch/nn/modules/activation.py:1266\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1252\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1253\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1254\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1263\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1264\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1266\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/python-notebook2/lib/python3.12/site-packages/torch/nn/functional.py:5451\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 4, 32]' is invalid for input of size 384"
     ]
    }
   ],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50):\n",
    "    model.eval()\n",
    "    tokens = [token.lower() for token in list(sentence)]\n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "    src_mask = model.transformer.generate_square_subsequent_mask(src_tensor.size(0)).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_src = model.transformer.encoder(model.positional_encoding(model.src_embedding(src_tensor)), src_mask)\n",
    "\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "        trg_mask = model.transformer.generate_square_subsequent_mask(trg_tensor.size(0)).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model.transformer.decoder(model.positional_encoding(model.trg_embedding(trg_tensor)), enc_src, trg_mask)\n",
    "            output = model.fc_out(output)\n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        trg_indexes.append(pred_token)\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "\n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "\n",
    "    return trg_tokens[1:]\n",
    "\n",
    "# Example usage\n",
    "sentence = \"2+2\"\n",
    "translation = translate_sentence(sentence, SRC, TRG, model, device)\n",
    "print(\" \".join(translation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844a5517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28cfc58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0478a120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41c6d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1567f4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        output = model(data)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        loss = criterion(output_flat, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            seq_len = data.size(0)\n",
    "            output = model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc28658",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "    model.load_state_dict(torch.load(best_model_params_path)) # load best model states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb830ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate(model, test_data)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb7ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ca00b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ls/5qzq87350jz6pv6jwnpg6mkm0000gn/T/ipykernel_31004/526140221.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  a, b, c = model(torch.tensor(x_dummy))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized values - a: 3.8143, b: 6.0970, c: 0.0905\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.a = nn.Linear(hidden_size, 1)\n",
    "        self.b = nn.Linear(hidden_size, 1)\n",
    "        self.c = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.a_slope = nn.Parameter(torch.tensor(1.0))\n",
    "        self.a_intercept = nn.Parameter(torch.tensor(0.0))\n",
    "        self.b_slope = nn.Parameter(torch.tensor(1.0))\n",
    "        self.b_intercept = nn.Parameter(torch.tensor(0.0))\n",
    "        self.c_slope = nn.Parameter(torch.tensor(1.0))\n",
    "        self.c_intercept = nn.Parameter(torch.tensor(0.0))\n",
    "        self.relu= nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        # ao, bo, co = self.a(x), self.b(x), self.c(x)\n",
    "        # return ao, bo, co\n",
    "        a_base, b_base, c_base = x[:, 0], x[:, 1], x[:, 2]\n",
    "        a = self.a_slope * a_base + self.a_intercept\n",
    "        b = self.b_slope * b_base + self.b_intercept\n",
    "        c = self.c_slope * c_base + self.c_intercept\n",
    "        return a, b, c\n",
    "\n",
    "# Objective function\n",
    "def loss1(a, b, c):\n",
    "    return -(a * 2 + b * 5 + c * 3).mean()\n",
    "\n",
    "def loss2(a, b, c):\n",
    "    return (1000. * (a + b + c - 10) ** 2).mean()\n",
    "\n",
    "def loss3(a, b, c):\n",
    "    penalty = torch.relu(-a) + torch.relu(-b) + torch.relu(-c)\n",
    "    return penalty.mean() * 1000.  # Large penalty to enforce non-negativity\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleNet(1, 128)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    x_dummy = torch.zeros(100, 1)\n",
    "    a, b, c = model(torch.tensor(x_dummy))\n",
    "    #  = outputs[0][0], outputs[0][1], outputs[0][2]\n",
    "    loss = loss1(a, b, c) + loss2(a, b, c) + loss3(a, b, c)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # if (epoch + 1) % 100 == 0:\n",
    "        # print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, a: {a.item():.4f}, b: {b.item():.4f}, c: {c.item():.4f}')\n",
    "\n",
    "# Final optimized values\n",
    "a_opt, b_opt, c_opt = model(torch.tensor([[0.0]]))\n",
    "print(f'Optimized values - a: {a_opt.item():.4f}, b: {b_opt.item():.4f}, c: {c_opt.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a23443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Combined Loss: -35.9946, Objective Loss: -36.1386, Constraint Loss: 0.1440, Penalty: 0.0000, a: 1.2982, b: 3.7364, c: 4.9534\n",
      "Optimized values - a: 1.3030, b: 3.7480, c: 4.9679\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the encoder-decoder model\n",
    "class EncoderDecoderModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size):\n",
    "        super(EncoderDecoderModel, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, latent_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 3)  # Output 3 values for a, b, and c\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        outputs = self.decoder(latent)\n",
    "        a, b, c = outputs[:, 0], outputs[:, 1], outputs[:, 2]\n",
    "        return a, b, c\n",
    "\n",
    "# Objective function\n",
    "def loss1(a, b, c):\n",
    "    return -(a * 2 + b * 5 + c * 3).mean()\n",
    "\n",
    "def loss2(a, b, c):\n",
    "    return ((a + b + c - 10) ** 2).mean() * 1000.\n",
    "\n",
    "def loss3(a, b, c):\n",
    "    penalty = torch.relu(-a) + torch.relu(-b) + torch.relu(-c)\n",
    "    return penalty.mean() * 1000.  # Large penalty to enforce non-negativity\n",
    "\n",
    "# Initialize the model\n",
    "input_size = 1  # Dummy input size, since we're directly optimizing a, b, and c\n",
    "hidden_size = 10\n",
    "latent_size = 5\n",
    "model = EncoderDecoderModel(input_size, hidden_size, latent_size)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "batch_size = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Dummy input for the batch (all ones to simulate intercept learning)\n",
    "    x_dummy = torch.ones(batch_size, input_size)\n",
    "    \n",
    "    # Forward pass\n",
    "    a, b, c = model(x_dummy)\n",
    "    \n",
    "    # Compute losses\n",
    "    obj_loss = loss1(a, b, c)\n",
    "    const_loss = loss2(a, b, c)\n",
    "    non_neg_penalty = loss3(a, b, c)\n",
    "    \n",
    "    # Combine losses with weights\n",
    "    combined_loss = obj_loss + const_loss + non_neg_penalty\n",
    "    combined_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Combined Loss: {combined_loss.item():.4f}, Objective Loss: {obj_loss.item():.4f}, Constraint Loss: {const_loss.item():.4f}, Penalty: {non_neg_penalty.item():.4f}, a: {a.mean().item():.4f}, b: {b.mean().item():.4f}, c: {c.mean().item():.4f}')\n",
    "\n",
    "# Final optimized values (using a single input to get final values)\n",
    "x_dummy = torch.ones(1, input_size)  # Single input to get final values\n",
    "a_opt, b_opt, c_opt = model(x_dummy)\n",
    "print(f'Optimized values - a: {a_opt.item():.4f}, b: {b_opt.item():.4f}, c: {c_opt.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0938275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/10000], Combined Loss: -23.5286, Objective Loss: -34.0993, Constraint Loss: 10.5707, Penalty: 0.0000, a: 3.0835, b: 3.4372, c: 3.5821\n",
      "Epoch [200/10000], Combined Loss: -33.7676, Objective Loss: -33.7729, Constraint Loss: 0.0054, Penalty: 0.0000, a: 3.0488, b: 3.4074, c: 3.5461\n",
      "Epoch [300/10000], Combined Loss: -33.7813, Objective Loss: -33.7841, Constraint Loss: 0.0028, Penalty: 0.0000, a: 3.0447, b: 3.4119, c: 3.5450\n",
      "Epoch [400/10000], Combined Loss: -33.7975, Objective Loss: -33.8003, Constraint Loss: 0.0028, Penalty: 0.0000, a: 3.0401, b: 3.4177, c: 3.5439\n",
      "Epoch [500/10000], Combined Loss: -33.8164, Objective Loss: -33.8192, Constraint Loss: 0.0028, Penalty: 0.0000, a: 3.0346, b: 3.4244, c: 3.5427\n",
      "Epoch [600/10000], Combined Loss: -33.8378, Objective Loss: -33.8406, Constraint Loss: 0.0028, Penalty: 0.0000, a: 3.0284, b: 3.4320, c: 3.5413\n",
      "Epoch [700/10000], Combined Loss: -33.8618, Objective Loss: -33.8646, Constraint Loss: 0.0028, Penalty: 0.0000, a: 3.0214, b: 3.4405, c: 3.5397\n",
      "Epoch [800/10000], Combined Loss: -33.8883, Objective Loss: -33.8911, Constraint Loss: 0.0028, Penalty: 0.0000, a: 3.0138, b: 3.4499, c: 3.5380\n",
      "Epoch [900/10000], Combined Loss: -33.9173, Objective Loss: -33.9201, Constraint Loss: 0.0028, Penalty: 0.0000, a: 3.0054, b: 3.4602, c: 3.5361\n",
      "Epoch [1000/10000], Combined Loss: -33.9488, Objective Loss: -33.9517, Constraint Loss: 0.0028, Penalty: 0.0000, a: 2.9962, b: 3.4714, c: 3.5340\n",
      "Epoch [1100/10000], Combined Loss: -33.9830, Objective Loss: -33.9858, Constraint Loss: 0.0028, Penalty: 0.0000, a: 2.9863, b: 3.4836, c: 3.5318\n",
      "Epoch [1200/10000], Combined Loss: -34.0199, Objective Loss: -34.0227, Constraint Loss: 0.0028, Penalty: 0.0000, a: 2.9757, b: 3.4967, c: 3.5294\n",
      "Epoch [1300/10000], Combined Loss: -34.0595, Objective Loss: -34.0623, Constraint Loss: 0.0028, Penalty: 0.0000, a: 2.9642, b: 3.5107, c: 3.5268\n",
      "Epoch [1400/10000], Combined Loss: -34.1019, Objective Loss: -34.1048, Constraint Loss: 0.0028, Penalty: 0.0000, a: 2.9519, b: 3.5258, c: 3.5240\n",
      "Epoch [1500/10000], Combined Loss: -34.1473, Objective Loss: -34.1502, Constraint Loss: 0.0028, Penalty: 0.0000, a: 2.9387, b: 3.5419, c: 3.5211\n",
      "Epoch [1600/10000], Combined Loss: -34.1958, Objective Loss: -34.1986, Constraint Loss: 0.0029, Penalty: 0.0000, a: 2.9247, b: 3.5591, c: 3.5179\n",
      "Epoch [1700/10000], Combined Loss: -34.2474, Objective Loss: -34.2502, Constraint Loss: 0.0029, Penalty: 0.0000, a: 2.9097, b: 3.5774, c: 3.5145\n",
      "Epoch [1800/10000], Combined Loss: -34.3023, Objective Loss: -34.3051, Constraint Loss: 0.0029, Penalty: 0.0000, a: 2.8938, b: 3.5969, c: 3.5110\n",
      "Epoch [1900/10000], Combined Loss: -34.3606, Objective Loss: -34.3635, Constraint Loss: 0.0029, Penalty: 0.0000, a: 2.8769, b: 3.6176, c: 3.5072\n",
      "Epoch [2000/10000], Combined Loss: -34.4226, Objective Loss: -34.4254, Constraint Loss: 0.0029, Penalty: 0.0000, a: 2.8589, b: 3.6396, c: 3.5032\n",
      "Epoch [2100/10000], Combined Loss: -34.4883, Objective Loss: -34.4911, Constraint Loss: 0.0029, Penalty: 0.0000, a: 2.8398, b: 3.6629, c: 3.4989\n",
      "Epoch [2200/10000], Combined Loss: -34.5579, Objective Loss: -34.5608, Constraint Loss: 0.0029, Penalty: 0.0000, a: 2.8196, b: 3.6877, c: 3.4944\n",
      "Epoch [2300/10000], Combined Loss: -34.6317, Objective Loss: -34.6346, Constraint Loss: 0.0029, Penalty: 0.0000, a: 2.7982, b: 3.7139, c: 3.4896\n",
      "Epoch [2400/10000], Combined Loss: -34.7098, Objective Loss: -34.7128, Constraint Loss: 0.0029, Penalty: 0.0000, a: 2.7755, b: 3.7416, c: 3.4846\n",
      "Epoch [2500/10000], Combined Loss: -34.7926, Objective Loss: -34.7955, Constraint Loss: 0.0029, Penalty: 0.0000, a: 2.7515, b: 3.7709, c: 3.4793\n",
      "Epoch [2600/10000], Combined Loss: -34.8801, Objective Loss: -34.8830, Constraint Loss: 0.0029, Penalty: 0.0000, a: 2.7261, b: 3.8020, c: 3.4737\n",
      "Epoch [2700/10000], Combined Loss: -34.9727, Objective Loss: -34.9757, Constraint Loss: 0.0029, Penalty: 0.0000, a: 2.6992, b: 3.8348, c: 3.4677\n",
      "Epoch [2800/10000], Combined Loss: -35.0707, Objective Loss: -35.0737, Constraint Loss: 0.0030, Penalty: 0.0000, a: 2.6707, b: 3.8696, c: 3.4615\n",
      "Epoch [2900/10000], Combined Loss: -35.1744, Objective Loss: -35.1773, Constraint Loss: 0.0030, Penalty: 0.0000, a: 2.6405, b: 3.9063, c: 3.4549\n",
      "Epoch [3000/10000], Combined Loss: -35.2840, Objective Loss: -35.2870, Constraint Loss: 0.0030, Penalty: 0.0000, a: 2.6086, b: 3.9452, c: 3.4479\n",
      "Epoch [3100/10000], Combined Loss: -35.4001, Objective Loss: -35.4031, Constraint Loss: 0.0030, Penalty: 0.0000, a: 2.5749, b: 3.9864, c: 3.4405\n",
      "Epoch [3200/10000], Combined Loss: -35.5229, Objective Loss: -35.5259, Constraint Loss: 0.0030, Penalty: 0.0000, a: 2.5391, b: 4.0299, c: 3.4327\n",
      "Epoch [3300/10000], Combined Loss: -35.6529, Objective Loss: -35.6560, Constraint Loss: 0.0030, Penalty: 0.0000, a: 2.5012, b: 4.0760, c: 3.4245\n",
      "Epoch [3400/10000], Combined Loss: -35.7906, Objective Loss: -35.7937, Constraint Loss: 0.0030, Penalty: 0.0000, a: 2.4611, b: 4.1248, c: 3.4158\n",
      "Epoch [3500/10000], Combined Loss: -35.9366, Objective Loss: -35.9396, Constraint Loss: 0.0031, Penalty: 0.0000, a: 2.4186, b: 4.1765, c: 3.4067\n",
      "Epoch [3600/10000], Combined Loss: -36.0913, Objective Loss: -36.0943, Constraint Loss: 0.0031, Penalty: 0.0000, a: 2.3735, b: 4.2313, c: 3.3970\n",
      "Epoch [3700/10000], Combined Loss: -36.2554, Objective Loss: -36.2585, Constraint Loss: 0.0031, Penalty: 0.0000, a: 2.3256, b: 4.2894, c: 3.3868\n",
      "Epoch [3800/10000], Combined Loss: -36.4297, Objective Loss: -36.4328, Constraint Loss: 0.0031, Penalty: 0.0000, a: 2.2747, b: 4.3511, c: 3.3759\n",
      "Epoch [3900/10000], Combined Loss: -36.6149, Objective Loss: -36.6180, Constraint Loss: 0.0031, Penalty: 0.0000, a: 2.2206, b: 4.4167, c: 3.3644\n",
      "Epoch [4000/10000], Combined Loss: -36.8119, Objective Loss: -36.8151, Constraint Loss: 0.0032, Penalty: 0.0000, a: 2.1631, b: 4.4864, c: 3.3523\n",
      "Epoch [4100/10000], Combined Loss: -37.0218, Objective Loss: -37.0250, Constraint Loss: 0.0032, Penalty: 0.0000, a: 2.1017, b: 4.5607, c: 3.3394\n",
      "Epoch [4200/10000], Combined Loss: -37.2457, Objective Loss: -37.2489, Constraint Loss: 0.0032, Penalty: 0.0000, a: 2.0362, b: 4.6399, c: 3.3257\n",
      "Epoch [4300/10000], Combined Loss: -37.4849, Objective Loss: -37.4881, Constraint Loss: 0.0032, Penalty: 0.0000, a: 1.9662, b: 4.7245, c: 3.3111\n",
      "Epoch [4400/10000], Combined Loss: -37.7409, Objective Loss: -37.7442, Constraint Loss: 0.0033, Penalty: 0.0000, a: 1.8912, b: 4.8150, c: 3.2956\n",
      "Epoch [4500/10000], Combined Loss: -38.0154, Objective Loss: -38.0187, Constraint Loss: 0.0033, Penalty: 0.0000, a: 1.8108, b: 4.9120, c: 3.2790\n",
      "Epoch [4600/10000], Combined Loss: -38.3105, Objective Loss: -38.3139, Constraint Loss: 0.0033, Penalty: 0.0000, a: 1.7242, b: 5.0163, c: 3.2613\n",
      "Epoch [4700/10000], Combined Loss: -38.6286, Objective Loss: -38.6320, Constraint Loss: 0.0034, Penalty: 0.0000, a: 1.6309, b: 5.1287, c: 3.2423\n",
      "Epoch [4800/10000], Combined Loss: -38.9724, Objective Loss: -38.9758, Constraint Loss: 0.0034, Penalty: 0.0000, a: 1.5299, b: 5.2501, c: 3.2218\n",
      "Epoch [4900/10000], Combined Loss: -39.3453, Objective Loss: -39.3487, Constraint Loss: 0.0034, Penalty: 0.0000, a: 1.4203, b: 5.3818, c: 3.1998\n",
      "Epoch [5000/10000], Combined Loss: -39.7514, Objective Loss: -39.7548, Constraint Loss: 0.0035, Penalty: 0.0000, a: 1.3009, b: 5.5251, c: 3.1759\n",
      "Epoch [5100/10000], Combined Loss: -40.1955, Objective Loss: -40.1990, Constraint Loss: 0.0035, Penalty: 0.0000, a: 1.1702, b: 5.6818, c: 3.1499\n",
      "Epoch [5200/10000], Combined Loss: -40.6837, Objective Loss: -40.6873, Constraint Loss: 0.0036, Penalty: 0.0000, a: 1.0264, b: 5.8540, c: 3.1216\n",
      "Epoch [5300/10000], Combined Loss: -41.2236, Objective Loss: -41.2272, Constraint Loss: 0.0036, Penalty: 0.0000, a: 0.8672, b: 6.0443, c: 3.0904\n",
      "Epoch [5400/10000], Combined Loss: -41.8246, Objective Loss: -41.8282, Constraint Loss: 0.0037, Penalty: 0.0000, a: 0.6899, b: 6.2562, c: 3.0558\n",
      "Epoch [5500/10000], Combined Loss: -42.4989, Objective Loss: -42.5026, Constraint Loss: 0.0037, Penalty: 0.0000, a: 0.4908, b: 6.4938, c: 3.0173\n",
      "Epoch [5600/10000], Combined Loss: -43.2201, Objective Loss: -43.2374, Constraint Loss: 0.0173, Penalty: 0.0000, a: 0.2658, b: 6.7578, c: 2.9722\n",
      "Epoch [5700/10000], Combined Loss: -44.1202, Objective Loss: -44.1241, Constraint Loss: 0.0039, Penalty: 0.0000, a: 0.0117, b: 7.0649, c: 2.9254\n",
      "Epoch [5800/10000], Combined Loss: -42.6074, Objective Loss: -42.6115, Constraint Loss: 0.0041, Penalty: 0.0000, a: 0.7919, b: 6.6987, c: 2.5115\n",
      "Epoch [5900/10000], Combined Loss: -43.3714, Objective Loss: -43.3752, Constraint Loss: 0.0039, Penalty: 0.0000, a: 0.5841, b: 6.9767, c: 2.4412\n",
      "Epoch [6000/10000], Combined Loss: -44.2460, Objective Loss: -44.2499, Constraint Loss: 0.0040, Penalty: 0.0000, a: 0.3464, b: 7.2952, c: 2.3603\n",
      "Epoch [6100/10000], Combined Loss: -45.2605, Objective Loss: -45.2645, Constraint Loss: 0.0040, Penalty: 0.0000, a: 0.0711, b: 7.6648, c: 2.2661\n",
      "Epoch [6200/10000], Combined Loss: -43.6871, Objective Loss: -43.7038, Constraint Loss: 0.0167, Penalty: 0.0000, a: 0.8838, b: 7.2877, c: 1.8326\n",
      "Epoch [6300/10000], Combined Loss: -44.5955, Objective Loss: -44.5996, Constraint Loss: 0.0041, Penalty: 0.0000, a: 0.6599, b: 7.6267, c: 1.7154\n",
      "Epoch [6400/10000], Combined Loss: -45.6492, Objective Loss: -45.6533, Constraint Loss: 0.0042, Penalty: 0.0000, a: 0.4002, b: 8.0237, c: 1.5781\n",
      "Epoch [6500/10000], Combined Loss: -46.8971, Objective Loss: -46.9013, Constraint Loss: 0.0042, Penalty: 0.0000, a: 0.0938, b: 8.4945, c: 1.4138\n",
      "Epoch [6600/10000], Combined Loss: -45.2333, Objective Loss: -45.2333, Constraint Loss: 0.0000, Penalty: 0.0000, a: 0.9590, b: 8.0964, c: 0.9445\n",
      "Epoch [6700/10000], Combined Loss: -46.3586, Objective Loss: -46.3629, Constraint Loss: 0.0043, Penalty: 0.0000, a: 0.7112, b: 8.5339, c: 0.7570\n",
      "Epoch [6800/10000], Combined Loss: -47.6973, Objective Loss: -47.7016, Constraint Loss: 0.0043, Penalty: 0.0000, a: 0.4165, b: 9.0559, c: 0.5297\n",
      "Epoch [6900/10000], Combined Loss: -49.3307, Objective Loss: -49.3351, Constraint Loss: 0.0044, Penalty: 0.0000, a: 0.0593, b: 9.6941, c: 0.2487\n",
      "Epoch [7000/10000], Combined Loss: -45.5846, Objective Loss: -45.5872, Constraint Loss: 0.0026, Penalty: 0.0000, a: 0.8016, b: 8.1968, c: 0.9999\n",
      "Epoch [7100/10000], Combined Loss: -46.5351, Objective Loss: -46.5395, Constraint Loss: 0.0044, Penalty: 0.0000, a: 0.5737, b: 8.5535, c: 0.8750\n",
      "Epoch [7200/10000], Combined Loss: -47.6304, Objective Loss: -47.6350, Constraint Loss: 0.0045, Penalty: 0.0000, a: 0.3078, b: 8.9682, c: 0.7261\n",
      "Epoch [7300/10000], Combined Loss: -35.8340, Objective Loss: -49.1189, Constraint Loss: 13.2849, Penalty: 0.0000, a: 0.1220, b: 9.4476, c: 0.5457\n",
      "Epoch [7400/10000], Combined Loss: -47.1579, Objective Loss: -47.1606, Constraint Loss: 0.0027, Penalty: 0.0000, a: 0.8279, b: 8.9918, c: 0.1820\n",
      "Epoch [7500/10000], Combined Loss: -46.3131, Objective Loss: -47.0737, Constraint Loss: 0.7606, Penalty: 0.0000, a: 0.4376, b: 8.7970, c: 0.7379\n",
      "Epoch [7600/10000], Combined Loss: -47.6796, Objective Loss: -47.6827, Constraint Loss: 0.0031, Penalty: 0.0000, a: 0.1187, b: 8.8981, c: 0.9850\n",
      "Epoch [7700/10000], Combined Loss: -46.0974, Objective Loss: -46.1041, Constraint Loss: 0.0067, Penalty: 0.0000, a: 0.8477, b: 8.4798, c: 0.6699\n",
      "Epoch [7800/10000], Combined Loss: -47.1173, Objective Loss: -47.1221, Constraint Loss: 0.0047, Penalty: 0.0000, a: 0.6153, b: 8.8654, c: 0.5214\n",
      "Epoch [7900/10000], Combined Loss: -48.2972, Objective Loss: -48.3019, Constraint Loss: 0.0047, Penalty: 0.0000, a: 0.3403, b: 9.3178, c: 0.3440\n",
      "Epoch [8000/10000], Combined Loss: -49.7122, Objective Loss: -49.7170, Constraint Loss: 0.0048, Penalty: 0.0000, a: 0.0122, b: 9.8613, c: 0.1286\n",
      "Epoch [8100/10000], Combined Loss: -46.1444, Objective Loss: -46.1480, Constraint Loss: 0.0036, Penalty: 0.0000, a: 0.7110, b: 8.4267, c: 0.8642\n",
      "Epoch [8200/10000], Combined Loss: -47.1129, Objective Loss: -47.1177, Constraint Loss: 0.0048, Penalty: 0.0000, a: 0.4750, b: 8.7931, c: 0.7341\n",
      "Epoch [8300/10000], Combined Loss: -48.2487, Objective Loss: -48.2535, Constraint Loss: 0.0048, Penalty: 0.0000, a: 0.1995, b: 9.2232, c: 0.5795\n",
      "Epoch [8400/10000], Combined Loss: -46.5834, Objective Loss: -46.6749, Constraint Loss: 0.0915, Penalty: 0.0000, a: 0.9183, b: 8.8110, c: 0.2612\n",
      "Epoch [8500/10000], Combined Loss: -47.7455, Objective Loss: -47.7499, Constraint Loss: 0.0045, Penalty: 0.0000, a: 0.6991, b: 9.2213, c: 0.0817\n",
      "Epoch [8600/10000], Combined Loss: -47.1362, Objective Loss: -47.1409, Constraint Loss: 0.0047, Penalty: 0.0000, a: 0.2966, b: 8.7220, c: 0.9793\n",
      "Epoch [8700/10000], Combined Loss: -48.2677, Objective Loss: -48.2725, Constraint Loss: 0.0048, Penalty: 0.0000, a: 0.0176, b: 9.1418, c: 0.8428\n",
      "Epoch [8800/10000], Combined Loss: -46.6351, Objective Loss: -46.6409, Constraint Loss: 0.0057, Penalty: 0.0000, a: 0.7531, b: 8.6934, c: 0.5560\n",
      "Epoch [8900/10000], Combined Loss: -47.7003, Objective Loss: -47.7052, Constraint Loss: 0.0049, Penalty: 0.0000, a: 0.5049, b: 9.1017, c: 0.3956\n",
      "Epoch [9000/10000], Combined Loss: -48.9636, Objective Loss: -48.9685, Constraint Loss: 0.0049, Penalty: 0.0000, a: 0.2122, b: 9.5870, c: 0.2030\n",
      "Epoch [9100/10000], Combined Loss: -45.7178, Objective Loss: -45.7401, Constraint Loss: 0.0224, Penalty: 0.0000, a: 0.8130, b: 8.2695, c: 0.9222\n",
      "Epoch [9200/10000], Combined Loss: -46.5446, Objective Loss: -46.5493, Constraint Loss: 0.0047, Penalty: 0.0000, a: 0.6046, b: 8.5737, c: 0.8239\n",
      "Epoch [9300/10000], Combined Loss: -47.6232, Objective Loss: -47.6281, Constraint Loss: 0.0050, Penalty: 0.0000, a: 0.3465, b: 8.9840, c: 0.6717\n",
      "Epoch [9400/10000], Combined Loss: -48.9064, Objective Loss: -48.9115, Constraint Loss: 0.0051, Penalty: 0.0000, a: 0.0410, b: 9.4729, c: 0.4884\n",
      "Epoch [9500/10000], Combined Loss: -47.1897, Objective Loss: -47.1911, Constraint Loss: 0.0014, Penalty: 0.0000, a: 0.8182, b: 9.0029, c: 0.1801\n",
      "Epoch [9600/10000], Combined Loss: -43.3267, Objective Loss: -47.4142, Constraint Loss: 4.0875, Penalty: 0.0000, a: 0.4741, b: 8.8483, c: 0.7415\n",
      "Epoch [9700/10000], Combined Loss: -47.6546, Objective Loss: -47.6601, Constraint Loss: 0.0056, Penalty: 0.0000, a: 0.1731, b: 8.9131, c: 0.9161\n",
      "Epoch [9800/10000], Combined Loss: -46.1583, Objective Loss: -46.2318, Constraint Loss: 0.0736, Penalty: 0.0000, a: 0.8451, b: 8.5256, c: 0.6379\n",
      "Epoch [9900/10000], Combined Loss: -47.1483, Objective Loss: -47.1536, Constraint Loss: 0.0052, Penalty: 0.0000, a: 0.6303, b: 8.8885, c: 0.4835\n",
      "Epoch [10000/10000], Combined Loss: -48.3067, Objective Loss: -48.3117, Constraint Loss: 0.0050, Penalty: 0.0000, a: 0.3646, b: 9.3348, c: 0.3028\n",
      "Optimized values - a: 0.3617, b: 9.3397, c: 0.3008\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the encoder-decoder model\n",
    "class EncoderDecoderModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size):\n",
    "        super(EncoderDecoderModel, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, latent_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 3)  # Output 3 values for a, b, and c\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        outputs = self.decoder(latent)\n",
    "        a, b, c = outputs[:, 0], outputs[:, 1], outputs[:, 2]\n",
    "        return a, b, c\n",
    "\n",
    "# Objective function\n",
    "def loss1(a, b, c):\n",
    "    return -(a * 2 + b * 5 + c * 3).mean()\n",
    "\n",
    "def loss2(a, b, c):\n",
    "    return ((a + b + c - 10) ** 2).mean() * 1000.\n",
    "\n",
    "def loss3(a, b, c):\n",
    "    penalty = torch.relu(-a) + torch.relu(-b) + torch.relu(-c)\n",
    "    return penalty.mean() * 1000.  # Large penalty to enforce non-negativity\n",
    "\n",
    "# Initialize the model\n",
    "input_size = 1  # Dummy input size, since we're directly optimizing a, b, and c\n",
    "hidden_size = 10\n",
    "latent_size = 5\n",
    "model = EncoderDecoderModel(input_size, hidden_size, latent_size)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10000\n",
    "batch_size = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Dummy input for the batch (all ones to simulate intercept learning)\n",
    "    x_dummy = torch.ones(batch_size, input_size)\n",
    "    \n",
    "    # Forward pass\n",
    "    a, b, c = model(x_dummy)\n",
    "    \n",
    "    # Compute losses\n",
    "    obj_loss = loss1(a, b, c)\n",
    "    const_loss = loss2(a, b, c)\n",
    "    non_neg_penalty = loss3(a, b, c)\n",
    "    \n",
    "    # Combine losses with weights\n",
    "    combined_loss = obj_loss + const_loss + non_neg_penalty\n",
    "    combined_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Combined Loss: {combined_loss.item():.4f}, Objective Loss: {obj_loss.item():.4f}, Constraint Loss: {const_loss.item():.4f}, Penalty: {non_neg_penalty.item():.4f}, a: {a.mean().item():.4f}, b: {b.mean().item():.4f}, c: {c.mean().item():.4f}')\n",
    "\n",
    "# Final optimized values (using a single input to get final values)\n",
    "x_dummy = torch.ones(1, input_size)  # Single input to get final values\n",
    "a_opt, b_opt, c_opt = model(x_dummy)\n",
    "print(f'Optimized values - a: {a_opt.item():.4f}, b: {b_opt.item():.4f}, c: {c_opt.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b175b780",
   "metadata": {},
   "source": [
    "### Regression Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bf79e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.1263\n",
      "Epoch [200/1000], Loss: 0.0249\n",
      "Epoch [300/1000], Loss: 0.0082\n",
      "Epoch [400/1000], Loss: 0.0032\n",
      "Epoch [500/1000], Loss: 0.0049\n",
      "Epoch [600/1000], Loss: 0.0018\n",
      "Epoch [700/1000], Loss: 0.0046\n",
      "Epoch [800/1000], Loss: 0.0015\n",
      "Epoch [900/1000], Loss: 0.0015\n",
      "Epoch [1000/1000], Loss: 0.0019\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value: 3.0144801139831543 at X: 2.929292917251587\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate data for the optimization problem\n",
    "X = np.linspace(0, 5, 100).reshape(-1, 1)  # Features\n",
    "y = -X**2 + 4*X  # Labels (objective function values)\n",
    "y = np.where(y > 3, 3, y)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Define a simple neural network\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = 1\n",
    "hidden_size = 10\n",
    "model = NeuralNetwork(input_size, hidden_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted = model(X_tensor).numpy()\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X, y, label='Original Data')\n",
    "plt.plot(X, predicted, label='Fitted Line', color='red')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('f(X)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Find the optimal solution\n",
    "X_new = torch.tensor(np.linspace(0, 5, 100).reshape(-1, 1), dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_new)\n",
    "optimal_value, optimal_index = torch.max(predictions, 0)\n",
    "optimal_x = X_new[optimal_index]\n",
    "\n",
    "print(f'Optimal value: {optimal_value.item()} at X: {optimal_x.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec5f2b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb505c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0720\n",
      "Epoch [2/10], Loss: 0.0643\n",
      "Epoch [3/10], Loss: 0.0620\n",
      "Epoch [4/10], Loss: 0.0593\n",
      "Epoch [5/10], Loss: 0.0604\n",
      "Epoch [6/10], Loss: 0.0588\n",
      "Epoch [7/10], Loss: 0.0586\n",
      "Epoch [8/10], Loss: 0.0592\n",
      "Epoch [9/10], Loss: 0.0587\n",
      "Epoch [10/10], Loss: 0.0577\n",
      "Mean Squared Error (MSE): 0.0571\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.features = torch.tensor(X, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.output(x)  # No sigmoid here\n",
    "        return x\n",
    "\n",
    "\n",
    "data = {\n",
    "    'feature1': [1.0, 2.0, 3.0, 4.0],\n",
    "    'feature2': [5.0, 6.0, 7.0, 8.0],\n",
    "    'feature3': [9.0, 10.0, 11.0, 12.0],\n",
    "    'target': [0.0, 0.2, 0.4, 0.6]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split into features and targets\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# Create dataset and dataloader\n",
    "batch_size = 2\n",
    "dataset = CustomDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Instantiate the model, define the loss function and the optimizer\n",
    "input_size = X.shape[1]\n",
    "hidden_size = 5\n",
    "model = NeuralNetwork(input_size, hidden_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch_features, batch_targets in dataloader:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_features)\n",
    "        scaled_outputs = torch.sigmoid(outputs)  # Scale the output to [0, 1] here\n",
    "        loss = criterion(scaled_outputs, batch_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_epoch_loss:.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "val_dataset = CustomDataset(X, y)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "\n",
    "predicted_values = []\n",
    "target_values = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_features, batch_targets in val_dataloader:\n",
    "        outputs = model(batch_features)\n",
    "        scaled_outputs = torch.sigmoid(outputs)  # Scale the output to [0, 1] here\n",
    "        predicted_values.extend(scaled_outputs.numpy())\n",
    "        target_values.extend(batch_targets.numpy())\n",
    "\n",
    "# Compute MSE\n",
    "predicted_values = torch.tensor(predicted_values).view(-1)\n",
    "target_values = torch.tensor(target_values).view(-1)\n",
    "\n",
    "mse = mean_squared_error(target_values, predicted_values)\n",
    "print(f'Mean Squared Error (MSE): {mse:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
