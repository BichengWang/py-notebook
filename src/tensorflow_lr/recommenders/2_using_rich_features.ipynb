{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff32c734",
   "metadata": {},
   "source": [
    "# Feature preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb024502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bucketized_user_age': 45.0,\n",
      " 'movie_genres': array([7], dtype=int64),\n",
      " 'movie_id': b'357',\n",
      " 'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
      " 'raw_user_age': 46.0,\n",
      " 'timestamp': 879024327,\n",
      " 'user_gender': True,\n",
      " 'user_id': b'138',\n",
      " 'user_occupation_label': 4,\n",
      " 'user_occupation_text': b'doctor',\n",
      " 'user_rating': 4.0,\n",
      " 'user_zip_code': b'53211'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "\n",
    "for x in ratings.take(1).as_numpy_iterator():\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa96ca4d",
   "metadata": {},
   "source": [
    "### Categorical features into embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d03835ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "movie_title_lookup = tf.keras.layers.StringLookup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21e2b929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['[UNK]', 'Star Wars (1977)', 'Contact (1997)']\n"
     ]
    }
   ],
   "source": [
    "movie_title_lookup.adapt(ratings.map(lambda x: x[\"movie_title\"]))\n",
    "\n",
    "print(f\"Vocabulary: {movie_title_lookup.get_vocabulary()[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd44c1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([ 1, 58], dtype=int64)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_title_lookup([\"Star Wars (1977)\", \"One Flew Over the Cuckoo's Nest (1975)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff5919fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set up a large number of bins to reduce the chance of hash collisions.\n",
    "num_hashing_bins = 200_000\n",
    "\n",
    "movie_title_hashing = tf.keras.layers.Hashing(\n",
    "    num_bins=num_hashing_bins\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d07e5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([101016,  96565], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_title_hashing([\"Star Wars (1977)\", \"One Flew Over the Cuckoo's Nest (1975)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ae4422",
   "metadata": {},
   "source": [
    "#### Embeddings definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "546f845f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
     ]
    }
   ],
   "source": [
    "movie_title_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=movie_title_lookup.vocab_size(),\n",
    "    output_dim=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ca80228",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_title_model = tf.keras.Sequential([movie_title_lookup, movie_title_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "936f6ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs=['Star Wars (1977)']. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs=['Star Wars (1977)']. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n",
       "array([[ 0.00610381,  0.0199898 , -0.00529752,  0.00561236,  0.00504239,\n",
       "         0.01730869, -0.02592282,  0.00428874, -0.01569873, -0.02609544,\n",
       "         0.03421838,  0.00506601,  0.02429708, -0.00970216,  0.02343206,\n",
       "        -0.01074767,  0.02097772, -0.0414677 ,  0.01972109,  0.00086432,\n",
       "         0.03630659,  0.01607773, -0.0374319 ,  0.03371208,  0.01802197,\n",
       "        -0.04982349, -0.03293213, -0.01756442,  0.02507091, -0.04020079,\n",
       "        -0.02304267, -0.03547617]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_title_model([\"Star Wars (1977)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9be7d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
     ]
    }
   ],
   "source": [
    "user_id_lookup = tf.keras.layers.StringLookup()\n",
    "user_id_lookup.adapt(ratings.map(lambda x: x[\"user_id\"]))\n",
    "\n",
    "user_id_embedding = tf.keras.layers.Embedding(user_id_lookup.vocab_size(), 32)\n",
    "\n",
    "user_id_model = tf.keras.Sequential([user_id_lookup, user_id_embedding])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899cba68",
   "metadata": {},
   "source": [
    "### Continuous features normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0f2841a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 879024327.\n",
      "Timestamp: 875654590.\n",
      "Timestamp: 882075110.\n"
     ]
    }
   ],
   "source": [
    "for x in ratings.take(3).as_numpy_iterator():\n",
    "    print(f\"Timestamp: {x['timestamp']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fae156",
   "metadata": {},
   "source": [
    "#### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa9f7e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized timestamp: [-0.8429372].\n",
      "Normalized timestamp: [-1.4735202].\n",
      "Normalized timestamp: [-0.27203265].\n"
     ]
    }
   ],
   "source": [
    "timestamp_normalization = tf.keras.layers.Normalization(\n",
    "    axis=None\n",
    ")\n",
    "timestamp_normalization.adapt(ratings.map(lambda x: x[\"timestamp\"]).batch(1024))\n",
    "\n",
    "for x in ratings.take(3).as_numpy_iterator():\n",
    "    print(f\"Normalized timestamp: {timestamp_normalization(x['timestamp'])}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a48561",
   "metadata": {},
   "source": [
    "#### Discretizatio    \n",
    "Another common transformation is to turn a continuous feature into a number of categorical features. This makes good sense if we have reasons to suspect that a feature's effect is non-continuous.\n",
    "\n",
    "To do this, we first need to establish the boundaries of the buckets we will use for discretization. The easiest way is to identify the minimum and maximum value of the feature, and divide the resulting interval equally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41b04b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buckets: [8.74724710e+08 8.74743291e+08 8.74761871e+08]\n"
     ]
    }
   ],
   "source": [
    "max_timestamp = ratings.map(lambda x: x[\"timestamp\"]).reduce(\n",
    "    tf.cast(0, tf.int64), tf.maximum).numpy().max()\n",
    "min_timestamp = ratings.map(lambda x: x[\"timestamp\"]).reduce(\n",
    "    np.int64(1e9), tf.minimum).numpy().min()\n",
    "\n",
    "timestamp_buckets = np.linspace(\n",
    "    min_timestamp, max_timestamp, num=1000)\n",
    "\n",
    "print(f\"Buckets: {timestamp_buckets[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9e2e6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp embedding: [[-0.00444943 -0.03609366  0.00537453  0.02719212 -0.04922748 -0.04301534\n",
      "  -0.00922342 -0.00604397 -0.04302615  0.03931583 -0.0448388   0.00739118\n",
      "  -0.02025586 -0.03494602 -0.02490181 -0.0369978  -0.00393345 -0.03461321\n",
      "  -0.01413335 -0.01352978  0.03437234 -0.03381085 -0.03317273 -0.04448729\n",
      "  -0.02640278 -0.02385683  0.03025926  0.01867988  0.0499028   0.03835675\n",
      "   0.04041982  0.00054783]].\n"
     ]
    }
   ],
   "source": [
    "timestamp_embedding_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Discretization(timestamp_buckets.tolist()),\n",
    "  tf.keras.layers.Embedding(len(timestamp_buckets) + 1, 32)\n",
    "])\n",
    "\n",
    "for timestamp in ratings.take(1).map(lambda x: x[\"timestamp\"]).batch(1).as_numpy_iterator():\n",
    "    print(f\"Timestamp embedding: {timestamp_embedding_model(timestamp)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d92ce2f",
   "metadata": {},
   "source": [
    "### Processing text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c85159db",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_text = tf.keras.layers.TextVectorization()\n",
    "title_text.adapt(ratings.map(lambda x: x[\"movie_title\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ee519ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 32 266 162   2 267 265  53]], shape=(1, 7), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for row in ratings.batch(1).map(lambda x: x[\"movie_title\"]).take(1):\n",
    "    print(title_text(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04c99457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first', '1998', '1977', '1971', 'monty']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_text.get_vocabulary()[40:45]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d798c410",
   "metadata": {},
   "source": [
    "### Entity models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c3bcca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.user_embedding = tf.keras.Sequential([\n",
    "            user_id_lookup,\n",
    "            tf.keras.layers.Embedding(user_id_lookup.vocab_size(), 32),\n",
    "        ])\n",
    "        self.timestamp_embedding = tf.keras.Sequential([\n",
    "          tf.keras.layers.Discretization(timestamp_buckets.tolist()),\n",
    "          tf.keras.layers.Embedding(len(timestamp_buckets) + 2, 32)\n",
    "        ])\n",
    "        self.normalized_timestamp = tf.keras.layers.Normalization(\n",
    "            axis=None\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Take the input dictionary, pass it through each input layer,\n",
    "        # and concatenate the result.\n",
    "        return tf.concat([\n",
    "            self.user_embedding(inputs[\"user_id\"]),\n",
    "            self.timestamp_embedding(inputs[\"timestamp\"]),\n",
    "            tf.reshape(self.normalized_timestamp(inputs[\"timestamp\"]), (-1, 1))\n",
    "        ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fc52d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed representations: [-0.01049465  0.02520282 -0.03168577]\n"
     ]
    }
   ],
   "source": [
    "user_model = UserModel()\n",
    "\n",
    "user_model.normalized_timestamp.adapt(\n",
    "    ratings.map(lambda x: x[\"timestamp\"]).batch(128))\n",
    "\n",
    "for row in ratings.batch(1).take(1):\n",
    "    print(f\"Computed representations: {user_model(row)[0, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0588bce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        max_tokens = 10_000\n",
    "        self.title_embedding = tf.keras.Sequential([\n",
    "          movie_title_lookup,\n",
    "       tf.keras.layers.Embedding(movie_title_lookup.vocab_size(), 32)\n",
    "        ])\n",
    "        self.title_text_embedding = tf.keras.Sequential([\n",
    "          tf.keras.layers.TextVectorization(max_tokens=max_tokens),\n",
    "          tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "          # We average the embedding of individual words to get one embedding vector\n",
    "          # per title.\n",
    "          tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.concat([\n",
    "            self.title_embedding(inputs[\"movie_title\"]),\n",
    "            self.title_text_embedding(inputs[\"movie_title\"]),\n",
    "        ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7db31de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed representations: [-0.0181139   0.01279012 -0.03667275]\n"
     ]
    }
   ],
   "source": [
    "movie_model = MovieModel()\n",
    "\n",
    "movie_model.title_text_embedding.layers[0].adapt(\n",
    "    ratings.map(lambda x: x[\"movie_title\"]))\n",
    "\n",
    "for row in ratings.batch(1).take(1):\n",
    "    print(f\"Computed representations: {movie_model(row)[0, :3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8450146e",
   "metadata": {},
   "source": [
    "# Leverage Context Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03165543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08f39081",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")\n",
    "\n",
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "    \"timestamp\": x[\"timestamp\"],\n",
    "})\n",
    "movies = movies.map(lambda x: x[\"movie_title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2e19f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = np.concatenate(list(ratings.map(lambda x: x[\"timestamp\"]).batch(100)))\n",
    "\n",
    "max_timestamp = timestamps.max()\n",
    "min_timestamp = timestamps.min()\n",
    "\n",
    "timestamp_buckets = np.linspace(\n",
    "    min_timestamp, max_timestamp, num=1000,\n",
    ")\n",
    "\n",
    "unique_movie_titles = np.unique(np.concatenate(list(movies.batch(1000))))\n",
    "unique_user_ids = np.unique(np.concatenate(list(ratings.batch(1_000).map(\n",
    "    lambda x: x[\"user_id\"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f937f95",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae3d0df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
