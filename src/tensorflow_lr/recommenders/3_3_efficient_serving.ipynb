{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 05:32:16.400312: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-09 05:32:16.519851: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-09 05:32:16.525124: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl/lib:/usr/local/nvidia/lib64:/usr/local/lib/R/lib/\n",
      "2022-11-09 05:32:16.525145: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-09 05:32:16.552059: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-09 05:32:17.373693: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl/lib:/usr/local/nvidia/lib64:/usr/local/lib/R/lib/\n",
      "2022-11-09 05:32:17.373777: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl/lib:/usr/local/nvidia/lib64:/usr/local/lib/R/lib/\n",
      "2022-11-09 05:32:17.373785: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Text\n",
    "\n",
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 05:32:18.757023: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl/lib:/usr/local/nvidia/lib64:/usr/local/lib/R/lib/\n",
      "2022-11-09 05:32:18.757111: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl/lib:/usr/local/nvidia/lib64:/usr/local/lib/R/lib/\n",
      "2022-11-09 05:32:18.757160: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl/lib:/usr/local/nvidia/lib64:/usr/local/lib/R/lib/\n",
      "2022-11-09 05:32:18.757206: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl/lib:/usr/local/nvidia/lib64:/usr/local/lib/R/lib/\n",
      "2022-11-09 05:32:18.757252: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl/lib:/usr/local/nvidia/lib64:/usr/local/lib/R/lib/\n",
      "2022-11-09 05:32:18.757297: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl/lib:/usr/local/nvidia/lib64:/usr/local/lib/R/lib/\n",
      "2022-11-09 05:32:18.757341: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl/lib:/usr/local/nvidia/lib64:/usr/local/lib/R/lib/\n",
      "2022-11-09 05:32:18.757386: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl/lib:/usr/local/nvidia/lib64:/usr/local/lib/R/lib/\n",
      "2022-11-09 05:32:18.757393: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-11-09 05:32:18.757646: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Load the MovieLens 100K data.\n",
    "ratings = tfds.load(\n",
    "    \"movielens/100k-ratings\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "# Get the ratings data.\n",
    "ratings = (ratings\n",
    "           # Retain only the fields we need.\n",
    "           .map(lambda x: {\"user_id\": x[\"user_id\"], \"movie_title\": x[\"movie_title\"]})\n",
    "           # Cache for efficiency.\n",
    "           .cache(tempfile.NamedTemporaryFile().name)\n",
    ")\n",
    "\n",
    "# Get the movies data.\n",
    "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")\n",
    "movies = (movies\n",
    "          # Retain only the fields we need.\n",
    "          .map(lambda x: x[\"movie_title\"])\n",
    "          # Cache for efficiency.\n",
    "          .cache(tempfile.NamedTemporaryFile().name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 05:32:19.041837: W tensorflow/core/kernels/data/cache_dataset_ops.cc:296] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-11-09 05:32:21.229109: W tensorflow/core/kernels/data/cache_dataset_ops.cc:296] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "user_ids = ratings.map(lambda x: x[\"user_id\"])\n",
    "\n",
    "unique_movie_titles = np.unique(np.concatenate(list(movies.batch(1000))))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids.batch(1000))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovielensModel(tfrs.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        embedding_dimension = 32\n",
    "        # Set up a model for representing movies.\n",
    "        self.movie_model = tf.keras.Sequential([\n",
    "          tf.keras.layers.StringLookup(\n",
    "            vocabulary=unique_movie_titles, mask_token=None),\n",
    "          # We add an additional embedding to account for unknown tokens.\n",
    "          tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
    "        ])\n",
    "        # Set up a model for representing users.\n",
    "        self.user_model = tf.keras.Sequential([\n",
    "          tf.keras.layers.StringLookup(\n",
    "            vocabulary=unique_user_ids, mask_token=None),\n",
    "            # We add an additional embedding to account for unknown tokens.\n",
    "          tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
    "        ])\n",
    "        # Set up a task to optimize the model and compute metrics.\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "          metrics=tfrs.metrics.FactorizedTopK(\n",
    "            candidates=(\n",
    "                movies\n",
    "                .batch(128)\n",
    "                .cache()\n",
    "                .map(lambda title: (title, self.movie_model(title)))\n",
    "            )\n",
    "          )\n",
    "        )\n",
    "        \n",
    "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "        # We pick out the user features and pass them into the user model.\n",
    "        user_embeddings = self.user_model(features[\"user_id\"])\n",
    "        # And pick out the movie features and pass them into the movie model,\n",
    "        # getting embeddings back.\n",
    "        positive_movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "        # The task computes the loss and the metrics.\n",
    "        return self.task(\n",
    "            user_embeddings,\n",
    "            positive_movie_embeddings,\n",
    "            candidate_ids=features[\"movie_title\"],\n",
    "            compute_metrics=not training\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MovielensModel()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10/10 [==============================] - 2s 90ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 69848.8928 - regularization_loss: 0.0000e+00 - total_loss: 69848.8928\n",
      "Epoch 2/3\n",
      "10/10 [==============================] - 1s 91ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 67520.8125 - regularization_loss: 0.0000e+00 - total_loss: 67520.8125\n",
      "Epoch 3/3\n",
      "10/10 [==============================] - 1s 94ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 66307.8068 - regularization_loss: 0.0000e+00 - total_loss: 66307.8068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff940222990>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train.batch(8192), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 5s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0015 - factorized_top_k/top_5_categorical_accuracy: 0.0096 - factorized_top_k/top_10_categorical_accuracy: 0.0216 - factorized_top_k/top_50_categorical_accuracy: 0.1189 - factorized_top_k/top_100_categorical_accuracy: 0.2339 - loss: 49477.6260 - regularization_loss: 0.0000e+00 - total_loss: 49477.6260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.001500000013038516,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.009600000455975533,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.02160000056028366,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.11885000020265579,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.23385000228881836,\n",
       " 'loss': 28262.845703125,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 28262.845703125}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test.batch(8192), return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_recommenders.layers.factorized_top_k.BruteForce at 0x7ff9102becd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brute_force = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "brute_force.index_from_dataset(\n",
    "    movies.batch(128).map(lambda title: (title, model.movie_model(title)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top recommendations: [b'Rudy (1993)' b\"Kid in King Arthur's Court, A (1995)\"\n",
      " b'Homeward Bound: The Incredible Journey (1993)']\n"
     ]
    }
   ],
   "source": [
    "# Get predictions for user 42.\n",
    "_, titles = brute_force(np.array([\"42\"]), k=3)\n",
    "\n",
    "print(f\"Top recommendations: {titles[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.31 ms ± 12.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _, titles = brute_force(np.array([\"42\"]), k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a dataset of movies that's 1,000 times larger. We \n",
    "# do this by adding several million dummy movie titles to the dataset.\n",
    "lots_of_movies = tf.data.Dataset.concatenate(\n",
    "    movies.batch(4096),\n",
    "    movies.batch(4096).repeat(1_000).map(lambda x: tf.zeros_like(x))\n",
    ")\n",
    "\n",
    "# We also add lots of dummy embeddings by randomly perturbing\n",
    "# the estimated embeddings for real movies.\n",
    "lots_of_movies_embeddings = tf.data.Dataset.concatenate(\n",
    "    movies.batch(4096).map(model.movie_model),\n",
    "    movies.batch(4096).repeat(1_000)\n",
    "      .map(lambda x: model.movie_model(x))\n",
    "      .map(lambda x: x * tf.random.uniform(tf.shape(x)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_recommenders.layers.factorized_top_k.BruteForce at 0x7ff9303a08d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brute_force_lots = tfrs.layers.factorized_top_k.BruteForce()\n",
    "brute_force_lots.index_from_dataset(\n",
    "    tf.data.Dataset.zip((lots_of_movies, lots_of_movies_embeddings))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top recommendations: [b'Rudy (1993)' b\"Kid in King Arthur's Court, A (1995)\"\n",
      " b'Homeward Bound: The Incredible Journey (1993)']\n"
     ]
    }
   ],
   "source": [
    "_, titles = brute_force_lots(model.user_model(np.array([\"42\"])), k=3)\n",
    "\n",
    "print(f\"Top recommendations: {titles[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.82 ms ± 157 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _, titles = brute_force_lots(model.user_model(np.array([\"42\"])), k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 05:32:58.607233: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 100347\n",
      "2022-11-09 05:32:58.687784: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:88] PartitionerFactory ran in 80.462599ms.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_recommenders.layers.factorized_top_k.ScaNN at 0x7ff9302fb610>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scann = tfrs.layers.factorized_top_k.ScaNN(\n",
    "    num_reordering_candidates=500,\n",
    "    num_leaves_to_search=30\n",
    ")\n",
    "scann.index_from_dataset(\n",
    "    tf.data.Dataset.zip((lots_of_movies, lots_of_movies_embeddings))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top recommendations: [b'Rudy (1993)' b\"Kid in King Arthur's Court, A (1995)\"\n",
      " b'Homeward Bound: The Incredible Journey (1993)']\n"
     ]
    }
   ],
   "source": [
    "_, titles = scann(model.user_model(np.array([\"42\"])), k=3)\n",
    "\n",
    "print(f\"Top recommendations: {titles[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 ms ± 13.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _, titles = scann(model.user_model(np.array([\"42\"])), k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26min 4s, sys: 29.3 s, total: 26min 34s\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "# Override the existing streaming candidate source.\n",
    "model.task.factorized_metrics = tfrs.metrics.FactorizedTopK(\n",
    "    candidates=tf.data.Dataset.zip((lots_of_movies, lots_of_movies_embeddings))\n",
    ")\n",
    "# Need to recompile the model for the changes to take effect.\n",
    "model.compile()\n",
    "\n",
    "%time baseline_result = model.evaluate(test.batch(8192), return_dict=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.8 s, sys: 1.08 s, total: 21.9 s\n",
      "Wall time: 1.67 s\n"
     ]
    }
   ],
   "source": [
    "model.task.factorized_metrics = tfrs.metrics.FactorizedTopK(\n",
    "    candidates=scann\n",
    ")\n",
    "model.compile()\n",
    "\n",
    "# We can use a much bigger batch size here because ScaNN evaluation\n",
    "# is more memory efficient.\n",
    "%time scann_result = model.evaluate(test.batch(8192), return_dict=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brute force top-100 accuracy: 0.15\n",
      "ScaNN top-100 accuracy:       0.13\n"
     ]
    }
   ],
   "source": [
    "print(f\"Brute force top-100 accuracy: {baseline_result['factorized_top_k/top_100_categorical_accuracy']:.2f}\")\n",
    "print(f\"ScaNN top-100 accuracy:       {scann_result['factorized_top_k/top_100_categorical_accuracy']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the approximate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConcatenateDataset element_spec=TensorSpec(shape=(None, 32), dtype=tf.float32, name=None)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lots_of_movies_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 05:34:54.901860: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 100347\n",
      "2022-11-09 05:34:54.977946: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:88] PartitionerFactory ran in 76.0116ms.\n",
      "WARNING:absl:Found untraced functions such as query_with_exclusions while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpk26wunf1/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpk26wunf1/model/assets\n"
     ]
    }
   ],
   "source": [
    "# We re-index the ScaNN layer to include the user embeddings in the same model.\n",
    "# This way we can give the saved model raw features and get valid predictions\n",
    "# back.\n",
    "scann = tfrs.layers.factorized_top_k.ScaNN(model.user_model, num_reordering_candidates=1000)\n",
    "scann.index_from_dataset(\n",
    "    tf.data.Dataset.zip((lots_of_movies, lots_of_movies_embeddings))\n",
    ")\n",
    "\n",
    "# Need to call it to set the shapes.\n",
    "_ = scann(np.array([\"42\"]))\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmp:\n",
    "    path = os.path.join(tmp, \"model\")\n",
    "    tf.saved_model.save(\n",
    "        scann,\n",
    "        path,\n",
    "        options=tf.saved_model.SaveOptions(namespace_whitelist=[\"Scann\"])\n",
    "    )\n",
    "    loaded = tf.saved_model.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top recommendations: [b'Rudy (1993)' b\"Kid in King Arthur's Court, A (1995)\"\n",
      " b'Homeward Bound: The Incredible Journey (1993)']\n"
     ]
    }
   ],
   "source": [
    "_, titles = loaded(tf.constant([\"42\"]))\n",
    "\n",
    "print(f\"Top recommendations: {titles[0][:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning ScaNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process queries in groups of 1000; processing them all at once with brute force\n",
    "# may lead to out-of-memory errors, because processing a batch of q queries against\n",
    "# a size-n dataset takes O(nq) space with brute force.\n",
    "titles_ground_truth = tf.concat([\n",
    "  brute_force_lots(queries, k=10)[1] for queries in\n",
    "  test.batch(1000).map(lambda x: model.user_model(x[\"user_id\"]))\n",
    "], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all user_id's as a 1d tensor of strings\n",
    "test_flat = np.concatenate(list(test.map(lambda x: x[\"user_id\"]).batch(1000).as_numpy_iterator()), axis=0)\n",
    "\n",
    "# ScaNN is much more memory efficient and has no problem processing the whole\n",
    "# batch of 20000 queries at once.\n",
    "_, titles = scann(test_flat, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall(ground_truth, approx_results):\n",
    "    return np.mean([\n",
    "        len(np.intersect1d(truth, approx)) / len(truth)\n",
    "        for truth, approx in zip(ground_truth, approx_results)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.936\n"
     ]
    }
   ],
   "source": [
    "print(f\"Recall: {compute_recall(titles_ground_truth, titles):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.09 ms ± 38.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1000 scann(np.array([\"42\"]), k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 05:36:12.172058: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 100347\n",
      "2022-11-09 05:36:12.374992: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:88] PartitionerFactory ran in 202.851223ms.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.970\n"
     ]
    }
   ],
   "source": [
    "scann2 = tfrs.layers.factorized_top_k.ScaNN(\n",
    "    model.user_model, \n",
    "    num_leaves=1000,\n",
    "    num_leaves_to_search=100,\n",
    "    num_reordering_candidates=1000)\n",
    "scann2.index_from_dataset(\n",
    "    tf.data.Dataset.zip((lots_of_movies, lots_of_movies_embeddings))\n",
    ")\n",
    "\n",
    "_, titles2 = scann2(test_flat, k=10)\n",
    "\n",
    "print(f\"Recall: {compute_recall(titles_ground_truth, titles2):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19 ms ± 37 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1000 scann2(np.array([\"42\"]), k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 05:36:49.175864: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 100347\n",
      "2022-11-09 05:36:49.366556: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:88] PartitionerFactory ran in 190.614808ms.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.963\n"
     ]
    }
   ],
   "source": [
    "scann3 = tfrs.layers.factorized_top_k.ScaNN(\n",
    "    model.user_model,\n",
    "    num_leaves=1000,\n",
    "    num_leaves_to_search=70,\n",
    "    num_reordering_candidates=400)\n",
    "scann3.index_from_dataset(\n",
    "    tf.data.Dataset.zip((lots_of_movies, lots_of_movies_embeddings))\n",
    ")\n",
    "\n",
    "_, titles3 = scann3(test_flat, k=10)\n",
    "print(f\"Recall: {compute_recall(titles_ground_truth, titles3):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.98 ms ± 39.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1000 scann3(np.array([\"42\"]), k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01. Python 3.7 (General DS)",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
