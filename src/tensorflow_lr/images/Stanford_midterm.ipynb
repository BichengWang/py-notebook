{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "requested-marking",
   "metadata": {},
   "source": [
    "# CS230 Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-means",
   "metadata": {},
   "source": [
    "Name: Bicheng Wang   \n",
    "SUNETID: bichengw@stanford.edu   \n",
    "Signature: Bicheng Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-wings",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-problem",
   "metadata": {},
   "source": [
    "(a) iv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-france",
   "metadata": {},
   "source": [
    "(b) i, ii, iii, iv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-constant",
   "metadata": {},
   "source": [
    "(c) i, ii"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-think",
   "metadata": {},
   "source": [
    "(d) ii"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-radar",
   "metadata": {},
   "source": [
    "(e) iv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-climate",
   "metadata": {},
   "source": [
    "(f) iv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-contest",
   "metadata": {},
   "source": [
    "(g) iv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-peoples",
   "metadata": {},
   "source": [
    "(h) iv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-roots",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-earth",
   "metadata": {},
   "source": [
    "(a)    \n",
    "The model already accurately predicts all training data, no error need to backpropagate.   \n",
    "The model cannot rightly backpropagate the training error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-branch",
   "metadata": {},
   "source": [
    "(b)   \n",
    "The first one is we should avoid the false positive result, secondly, we also hope to push model performance. \n",
    "Thus, \n",
    "1.\tPrecision rate is one important metrics if we do not want to punish the good people.\n",
    "2.\tAt the same time, if we also hope to ensure the city safety, we need also ensure the Recall rate not too low, So choose F1 score could ensure both precision and recall rate as high as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-shirt",
   "metadata": {},
   "source": [
    "(c)   \n",
    "The benefit: a. increase model invariance, like translation, rotation, scale invariance. b. reduces whole information but focus on experienced important feature, improve the calculation efficiency, and reduce the overfit possibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-serve",
   "metadata": {},
   "source": [
    "(d)   \n",
    "High variance of model means it highly depend on specific data and low generalization ability and learn more noisy to make the model overfit. Possible way like consider drop out, batch norm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-virus",
   "metadata": {},
   "source": [
    "(e)   \n",
    "Small batch benefit like reduce calculation resource requirement in each batch train.   \n",
    "Disadvantage may have model convergence slow or not stable. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-citation",
   "metadata": {},
   "source": [
    "(f)   \n",
    "A. as softmax design for highly sensitive for higher value and the result as 0~1 possiblity, it ensure the max feature of final possibility result, B. Softmax easy to derive on backpropagation, especially combine with cross entropy, which often use in mutli-classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-margin",
   "metadata": {},
   "source": [
    "(g)   \n",
    "A. L1 is better on some case like as L1 sparse, it is a good way to select feature. B. L1 lower calculation performance than l2, as l2 good derive feature especially close to 0, and good at no-sparse vector calculation. C. l2 as MSE, the error > 1 outlier would be detecting more sensitive outlier, which also makes l2 has better performance than l1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-proportion",
   "metadata": {},
   "source": [
    "(h)   \n",
    "As downsampled the false class, model may not fully learn some part of false class feature. But no action needs to take on testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-contractor",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-degree",
   "metadata": {},
   "source": [
    "(i) HWC: 126, 126, 64, number of param: 1792"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-newman",
   "metadata": {},
   "source": [
    "(ii) HWC: 128 128 16 param: 1216"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-therapist",
   "metadata": {},
   "source": [
    "(iii) HWC: 64 64 32 param: 416"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-colorado",
   "metadata": {},
   "source": [
    "(iv) Output HWC: 14 14 64 total param: 75648"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-arthur",
   "metadata": {},
   "source": [
    "(v) Output HWC: 13 13 16 total param: 14048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-bhutan",
   "metadata": {},
   "source": [
    "(vi) Output HWC: 2 2 32 total param: 8672"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-disposal",
   "metadata": {},
   "source": [
    "(vii) The (vi) model. 128 to 10, said layer param: 1290"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-bargain",
   "metadata": {},
   "source": [
    "## Question 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-namibia",
   "metadata": {},
   "source": [
    "(i) The direvative func\n",
    "\\begin{equation}\n",
    "\\nabla_w \\widetilde{J} = \\nabla_w J + \\alpha \\cdot sign(w)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-setup",
   "metadata": {},
   "source": [
    "(ii) the original func could convert to:\n",
    "   \n",
    "\n",
    "\\begin{equation}\n",
    "J(w; X, y) = J(w^*; X, y) + sum_i(\\frac{1}{2}H_{i,i}(w_i - w_i^*)^2 + \\alpha_i|w_i|\n",
    "\\end{equation}\n",
    "\n",
    "And as the Hessian func we assume no correlation between features, and when gradient equal to zero, the derivative func by weight i could be:\n",
    "   \n",
    "\\begin{equation}\n",
    "H_{i,i}(w_i - w_i^*) + \\alpha \\cdot sign(w_i) = 0\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Then,\n",
    "\n",
    "\\begin{equation}\n",
    "w_i = sign(w^*) \\cdot (|w_i^*| - \\frac{\\alpha}{H_{i,i}})\n",
    "\\end{equation}\n",
    "\n",
    "Given condition 1:\n",
    "\\begin{equation}\n",
    "|w_i^*| - \\frac{\\alpha}{H_{i,i}} > 0\n",
    "\\end{equation}\n",
    "The new weight i update by:\n",
    "\\begin{equation}\n",
    "w_i = sign(w^*) \\cdot ||w_i^*| - \\frac{\\alpha}{H_{i,i}}|\n",
    "\\end{equation}\n",
    "\n",
    "Given condition 2:\n",
    "\\begin{equation}\n",
    "|w_i^*| - \\frac{\\alpha}{H_{i,i}} <= 0\n",
    "\\end{equation}\n",
    "The new weight i would be:\n",
    "\\begin{equation}\n",
    "w_i = 0\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-logging",
   "metadata": {},
   "source": [
    "(iii)    \n",
    "When large weight, the l1 update weight with a fixed number, while l2 update weight with a fixed number multiply weight value itself, if weight value > 1, the gradient faster.   \n",
    "When small weight, the l1 update a fixed number, while l2 update step smaller, gradient slower.    \n",
    "When weight close to 0, as l2 update a linear value with weight itself, l2 would more smooth to close to 0. While the l1 may not stable highly depend on the update constant value itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-currency",
   "metadata": {},
   "source": [
    "## Question 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-reliance",
   "metadata": {},
   "source": [
    "(i)   \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta_{d}} L (X; \\theta_d, \\theta_g) = \\frac{1}{n} \\sum_{i=1}^{n} (\\frac{\\nabla_{\\theta_d} D(X^i)}{D(X^i)} - \\frac{\\nabla_{\\theta_d}D(G(Z^i))}{1 - D(G(Z^i))})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-cisco",
   "metadata": {},
   "source": [
    "(ii)   \n",
    "According to the sigmoid derivation feature:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "g^{'}(x) = g(x)(1-g(x))\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "And combine with (i) function, the result(A, B are single simbols):\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L(\\theta_d, \\theta_g)}{\\partial z_d^{L_d}} = \\frac{1}{n} \\sum_{i=1}^{n} (\\frac{A^{'}}{A} + \\frac{-B^{'}}{1-B})\n",
    "\\end{equation}\n",
    "\n",
    "According to sigmoid, then:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L(\\theta_d, \\theta_g)}{\\partial z_d^{L_d}} = \\frac{1}{n} \\sum_{i=1}^{n} (1 - A - B)\n",
    "\\end{equation}\n",
    "\n",
    "Final combine with layer l activation values:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L(\\theta_d, \\theta_g)}{\\partial z_d^{L_d}} = \\frac{1}{n} \\sum_{i=1}^{n} (1 - 2A_d^{L_d})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-testimony",
   "metadata": {},
   "source": [
    "(iii)   \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L(\\theta_d, \\theta_g)}{\\partial z_d^{l}} =  W_d^{l+1} \\cdot  \\frac{\\partial L(\\theta_d, \\theta_g)}{\\partial z_d^{l+1}} \\cdot g_d^{'}(z_d^{l})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-marble",
   "metadata": {},
   "source": [
    "(iv)\n",
    "\n",
    "assume it is supplied post activation, g0 derivative is 1.\n",
    "Then,\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L(\\theta_d, \\theta_g)}{\\partial g(z_i; \\theta_g)} =  W_d^{1} \\cdot  \\frac{\\partial L(\\theta_d, \\theta_g)}{\\partial z_d^{1}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-baseline",
   "metadata": {},
   "source": [
    "(v)\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta_{g}} L(\\theta_d, \\theta_g) = \\frac{\\partial L(\\theta_d, \\theta_g)}{\\partial g(z_i; \\theta_g)} \\cdot \\nabla_{theta_g}g(z_i; \\theta_g)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-packaging",
   "metadata": {},
   "source": [
    "(vi)    \n",
    "Repeat{    \n",
    "&nbsp;Calculate the thetaD on t iteration and update with function:\n",
    "\\begin{equation}\n",
    "\\theta_d^{t+1} := \\theta_d^{t} - \\alpha \\cdot \\nabla_{theta_d}L(\\theta_d, \\theta_g)\n",
    "\\end{equation}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-affect",
   "metadata": {},
   "source": [
    "(vii)    \n",
    "Repeat{    \n",
    "&nbsp; Calculate the thetaG on t iteration and update with function:\n",
    "\\begin{equation}\n",
    "\\theta_g^{t+1} := \\theta_g^{t} - \\alpha \\cdot \\nabla_{theta_g}L(\\theta_d, \\theta_g)\n",
    "\\end{equation}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-border",
   "metadata": {},
   "source": [
    "(viii)\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta_{d}} L^{'}(\\theta_d, \\theta_g) = \\frac{1}{n} \\sum_{i=1}^{n} (\\nabla_{\\theta_d} D(X^i) + \\nabla_{\\theta_d} D(G(Z^i)) )\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-keyboard",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "h = 100\n",
    "w = 80\n",
    "input_channels = 3\n",
    "output_channels = 5\n",
    "\n",
    "key_kernel = (3,3,3)\n",
    "q_kernel=(1,1,3)\n",
    "value_kernel = (3,3,1)\n",
    "\n",
    "key_conv = Conv2d(input_channels, output_channels, kernel_size=(3,3))\n",
    "query_conv = Conv2d(input_channels, output_channels, kernel_size=1)\n",
    "value_conv = Conv2d(input_channels, output_channels, kernel_size=(3,3))\n",
    "\n",
    "def norm(X):\n",
    "    norm = np.linalg.norm(an_array)\n",
    "    return an_array/norm\n",
    "\n",
    "def forward(X):\n",
    "    key_out = key_conv(X)\n",
    "    query_out = query_conv(X[1:-1][1:-1][:])\n",
    "    value_out = value_conv(X)\n",
    "    softmax_input = numpy.zeros(h, w, output_channels)\n",
    "    hk, wk, ck = key_out.shape\n",
    "    for hi in range(hk):\n",
    "        for wi in range(wk):\n",
    "            for ci in range (ck):\n",
    "                softmax_input[hi, wi, ci] = np.multiply(key_out[hi,wi,ci], query_out[hi,hi,ci])\n",
    "    softmax_output = softmax(softmax_input)\n",
    "    out = numpy.zeros(ck)\n",
    "    return np.einsum(out, softmax_out)\n",
    "\n",
    "norm_X = norm(X)\n",
    "output = forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-printing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
